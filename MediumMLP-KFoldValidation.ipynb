{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import required libraries",
   "id": "88459d2d00efd6ec"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-27T00:22:39.384592Z",
     "start_time": "2025-11-27T00:22:38.219643Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, Subset\n",
    "\n",
    "# custom utility functions\n",
    "from modules.utils import Utils"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preliminary Setup",
   "id": "5d9a025eac01f071"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T00:22:39.425206Z",
     "start_time": "2025-11-27T00:22:39.391046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "utils = Utils()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "id": "997eb8ab6eef654c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exploring the Data",
   "id": "c7a9246972cb11b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T00:22:42.624193Z",
     "start_time": "2025-11-27T00:22:39.472554Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv('./data/train.csv')",
   "id": "405417de623b40a3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T00:22:42.647095Z",
     "start_time": "2025-11-27T00:22:42.629672Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "4bd9ff0945546e9b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   id  feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0   0   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "1   1   0.000000   0.000000   0.000000   0.071982   0.000000   0.000000   \n",
       "2   2   0.111111   0.000000   0.111111   0.000000   0.000000   0.111111   \n",
       "3   3   0.000000   0.087039   0.000000   0.000000   0.000000   0.000000   \n",
       "4   4   0.000000   0.000000   0.069673   0.000000   0.069673   0.000000   \n",
       "\n",
       "   feature_6  feature_7  feature_8  ...  feature_491  feature_492  \\\n",
       "0   0.000000   0.000000   0.000000  ...     0.000000     0.000000   \n",
       "1   0.000000   0.071982   0.000000  ...     0.000000     0.000000   \n",
       "2   0.000000   0.111111   0.000000  ...     0.000000     0.111111   \n",
       "3   0.000000   0.000000   0.087039  ...     0.000000     0.000000   \n",
       "4   0.069673   0.000000   0.000000  ...     0.069673     0.000000   \n",
       "\n",
       "   feature_493  feature_494  feature_495  feature_496  feature_497  \\\n",
       "0          0.0          0.0          0.0          0.0     0.000000   \n",
       "1          0.0          0.0          0.0          0.0     0.071982   \n",
       "2          0.0          0.0          0.0          0.0     0.000000   \n",
       "3          0.0          0.0          0.0          0.0     0.000000   \n",
       "4          0.0          0.0          0.0          0.0     0.000000   \n",
       "\n",
       "   feature_498  feature_499  label  \n",
       "0          0.0     0.063888     43  \n",
       "1          0.0     0.000000     16  \n",
       "2          0.0     0.000000     21  \n",
       "3          0.0     0.000000      2  \n",
       "4          0.0     0.000000      1  \n",
       "\n",
       "[5 rows x 502 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_491</th>\n",
       "      <th>feature_492</th>\n",
       "      <th>feature_493</th>\n",
       "      <th>feature_494</th>\n",
       "      <th>feature_495</th>\n",
       "      <th>feature_496</th>\n",
       "      <th>feature_497</th>\n",
       "      <th>feature_498</th>\n",
       "      <th>feature_499</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063888</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 502 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T00:22:42.687067Z",
     "start_time": "2025-11-27T00:22:42.683477Z"
    }
   },
   "cell_type": "code",
   "source": "df['label'].head()",
   "id": "7ab3fbe7158433dd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    43\n",
       "1    16\n",
       "2    21\n",
       "3     2\n",
       "4     1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T00:22:42.784377Z",
     "start_time": "2025-11-27T00:22:42.734434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# verify for any rows with null values\n",
    "null_counts = df.isnull().sum()\n",
    "print(null_counts[null_counts > 0])"
   ],
   "id": "f3e17ba352ed7e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T00:22:43.451982Z",
     "start_time": "2025-11-27T00:22:42.792163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# drop 'id' and 'label' for features\n",
    "X_df = df.drop(columns=['id', 'label'])\n",
    "\n",
    "# extract labels separately\n",
    "y_df = df['label']\n",
    "\n",
    "print(\"Number of feature columns:\", X_df.shape[1])"
   ],
   "id": "fc317086b47b258a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature columns: 500\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T00:22:43.462817Z",
     "start_time": "2025-11-27T00:22:43.458740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# verifying class count distribution\n",
    "class_counts = y_df.value_counts().sort_index()\n",
    "print(class_counts)"
   ],
   "id": "f1c430c3bec9bca3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0     2309\n",
      "1     2306\n",
      "2     2307\n",
      "3     2309\n",
      "4     2309\n",
      "5     2309\n",
      "6     2310\n",
      "7     2307\n",
      "8     2304\n",
      "9     2309\n",
      "10    2307\n",
      "11    2307\n",
      "12    2310\n",
      "13    2308\n",
      "14    2307\n",
      "15    2303\n",
      "16    2305\n",
      "17    2309\n",
      "18    2309\n",
      "19    2309\n",
      "20    2310\n",
      "21    2307\n",
      "22    2310\n",
      "23    2309\n",
      "24    2309\n",
      "25    2304\n",
      "26    2308\n",
      "27    2310\n",
      "28    2309\n",
      "29    2310\n",
      "30    2310\n",
      "31    2308\n",
      "32    2310\n",
      "33    2310\n",
      "34    2309\n",
      "35    2309\n",
      "36    2307\n",
      "37    2305\n",
      "38    2309\n",
      "39    2307\n",
      "40    2310\n",
      "41    2310\n",
      "42    2308\n",
      "43    2308\n",
      "44    2306\n",
      "45    2310\n",
      "46    2307\n",
      "47    2305\n",
      "48    2310\n",
      "49    2309\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Processing Data before K-Fold",
   "id": "8225c5e280de1e69"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T00:22:43.591091Z",
     "start_time": "2025-11-27T00:22:43.509674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_tensor = torch.tensor(X_df.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_df, dtype=torch.long)\n",
    "\n",
    "print(X_tensor.shape)\n",
    "print(y_tensor.shape)"
   ],
   "id": "5dbdaa8cae323b5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([115406, 500])\n",
      "torch.Size([115406])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T00:22:43.999282Z",
     "start_time": "2025-11-27T00:22:43.597893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NORMALIZING WITH MEAN 0 AND UNIT VARIANCE\n",
    "mean = X_tensor.mean(dim=0, keepdim=True)\n",
    "std = X_tensor.std(dim=0, unbiased=False, keepdim=True)\n",
    "\n",
    "# avoid division by zero for constant columns\n",
    "std[std == 0] = 1.0\n",
    "\n",
    "X_tensor_norm = (X_tensor - mean) / std\n",
    "\n",
    "print(X_tensor_norm.shape)\n",
    "print(X_tensor_norm.mean(dim=0))  # should be ~0\n",
    "print(X_tensor_norm.std(dim=0))   # should be ~1"
   ],
   "id": "ec665d99faff4dee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([115406, 500])\n",
      "tensor([-6.8555e-08, -4.5615e-08,  1.3685e-08,  2.0196e-08, -1.8378e-08,\n",
      "        -7.7480e-08, -1.3883e-08, -2.7634e-08, -8.6405e-08, -8.4620e-09,\n",
      "        -2.9088e-08, -1.9965e-08, -2.1915e-08,  3.9732e-08, -3.6889e-08,\n",
      "         1.4544e-08,  3.8542e-08,  5.5532e-09,  3.7352e-08,  4.4128e-08,\n",
      "        -1.9998e-09, -4.4822e-08,  2.4857e-08,  2.0229e-08, -1.3883e-08,\n",
      "         4.0327e-09,  2.8030e-08,  3.0542e-08,  2.0890e-08, -8.6603e-09,\n",
      "         2.8890e-08, -1.3288e-08, -3.3848e-08,  4.3500e-08, -1.8048e-08,\n",
      "        -1.4263e-08,  1.2561e-09,  1.9833e-08, -2.1089e-08, -3.1468e-08,\n",
      "         2.1287e-08, -9.4701e-09,  1.9833e-09,  1.8709e-08, -3.8277e-08,\n",
      "         1.9568e-08, -3.9732e-08,  1.9172e-09, -5.5267e-08,  1.5271e-08,\n",
      "        -7.8670e-09,  2.0031e-08, -6.3465e-09, -2.1485e-09, -8.1645e-09,\n",
      "         2.6642e-08,  4.7202e-08,  6.0027e-08, -3.1865e-08, -1.6924e-08,\n",
      "         3.4972e-08, -4.0988e-08, -4.4954e-09, -6.6407e-08,  8.7661e-08,\n",
      "        -4.4095e-08, -2.3865e-08,  1.1239e-08,  3.5963e-08,  2.1419e-08,\n",
      "         4.2310e-08, -5.2094e-08, -8.4686e-08, -2.6576e-08, -3.6096e-08,\n",
      "         6.5613e-09,  5.3020e-08, -1.5536e-08,  6.9580e-09, -1.6858e-09,\n",
      "        -6.4523e-08, -1.2164e-08, -1.8511e-08,  1.8114e-08, -7.6885e-08,\n",
      "         2.5254e-08,  2.0725e-08, -1.4808e-08, -1.3222e-09,  3.4773e-08,\n",
      "         6.5283e-09, -6.2804e-09,  1.7387e-08, -4.8921e-09,  2.8361e-08,\n",
      "        -1.4015e-08, -8.2636e-10, -3.5765e-08, -6.1151e-09, -2.5783e-08,\n",
      "        -1.5866e-09, -3.4906e-08,  1.9172e-08,  1.3486e-08, -1.9502e-08,\n",
      "        -2.0560e-08,  2.2609e-08,  2.1155e-09, -2.9815e-08,  5.4077e-08,\n",
      "        -5.8837e-09,  6.6109e-09,  5.9829e-09, -4.6871e-08,  3.3914e-08,\n",
      "        -3.0278e-08,  1.8841e-08,  2.7237e-08, -2.6973e-08, -9.7842e-09,\n",
      "        -3.0609e-08,  3.2724e-08,  2.4394e-08,  1.8742e-08, -3.3055e-08,\n",
      "         2.1353e-08,  2.9617e-08,  6.0192e-08,  3.1732e-09, -5.0177e-08,\n",
      "        -7.7678e-09,  2.8129e-08,  4.5946e-08, -2.2576e-08,  4.6276e-09,\n",
      "         2.1419e-08, -2.2411e-08, -1.8511e-09,  1.7254e-08, -2.2312e-08,\n",
      "         5.3416e-08, -1.4808e-08, -3.9203e-08,  2.3799e-08, -2.9749e-08,\n",
      "        -1.2296e-08, -7.5364e-09, -2.3733e-08, -2.7568e-08, -5.3548e-08,\n",
      "         2.2675e-08, -3.9004e-08, -2.6774e-08, -3.8079e-08,  1.0710e-08,\n",
      "         9.3214e-09,  3.0385e-08, -3.1534e-08, -3.1204e-08, -7.6026e-09,\n",
      "        -3.0278e-08, -2.5121e-09, -1.2032e-08,  4.2178e-08, -4.2359e-08,\n",
      "         3.4112e-08, -5.5466e-08, -6.6109e-11, -1.7552e-08,  3.3253e-08,\n",
      "         4.1649e-08, -1.8246e-08,  4.5748e-08,  6.3200e-08, -1.4114e-08,\n",
      "        -8.6603e-09, -1.0049e-08,  2.3006e-08, -7.0208e-08, -2.8559e-08,\n",
      "        -2.4460e-08, -1.3172e-08, -5.6193e-09,  6.7233e-08,  1.9568e-08,\n",
      "         8.1314e-09, -2.4064e-08, -3.6426e-08,  2.8824e-08, -1.3371e-08,\n",
      "        -1.1404e-08,  4.0525e-08,  4.1764e-08, -3.4840e-08,  2.8030e-08,\n",
      "        -4.6276e-09,  2.6080e-08,  6.5118e-09, -2.1155e-08,  4.1153e-08,\n",
      "         2.8559e-08,  5.2226e-08, -3.2393e-09,  1.6197e-09,  2.5518e-08,\n",
      "        -8.5281e-09,  1.2891e-08, -2.7964e-08,  3.9666e-10,  2.1535e-08,\n",
      "        -6.1878e-08,  4.8921e-09,  2.2808e-08,  4.1781e-08, -2.5419e-08,\n",
      "        -2.9088e-08, -6.5613e-09,  6.9018e-08,  3.0939e-08,  4.1120e-08,\n",
      "         1.7849e-08, -6.5779e-09, -5.7118e-08, -4.2310e-08, -5.6854e-09,\n",
      "        -2.5915e-08, -9.3875e-09, -1.3916e-08,  2.6179e-08,  1.9072e-08,\n",
      "        -3.5699e-08, -3.6294e-08,  4.7599e-09, -1.1900e-08, -2.6642e-08,\n",
      "         4.1715e-08,  1.3420e-08,  3.7814e-08, -2.4593e-08,  1.7998e-08,\n",
      "         1.3222e-10,  3.7748e-08, -1.2296e-08, -4.3235e-08, -3.4707e-09,\n",
      "         1.1106e-08, -2.8691e-08,  2.9815e-08, -7.9331e-09,  5.3813e-08,\n",
      "         4.2442e-08,  2.0097e-08,  1.3883e-08, -3.9533e-08, -2.3799e-08,\n",
      "         1.0710e-08, -3.9930e-08, -1.3552e-08,  2.9286e-08, -3.5897e-08,\n",
      "        -4.4161e-08, -5.1301e-08, -1.6131e-08,  2.9419e-08, -4.8689e-08,\n",
      "         6.5448e-08, -4.7499e-08,  2.4328e-08, -6.1482e-09, -5.4672e-08,\n",
      "        -2.6146e-08, -2.9617e-08,  4.6805e-08,  4.1186e-08, -9.3049e-09,\n",
      "         1.3486e-08,  1.2693e-08,  5.4210e-09, -1.2032e-08,  7.3513e-08,\n",
      "        -1.3354e-08, -2.0890e-08, -2.6906e-08, -1.5734e-08,  2.9088e-09,\n",
      "         2.3337e-08,  2.5849e-08,  5.0309e-08,  1.0181e-08,  1.2131e-08,\n",
      "         3.2096e-08,  1.1437e-08, -1.6180e-08,  9.2553e-10, -4.3103e-08,\n",
      "         6.3465e-09, -6.6109e-11, -1.4875e-10, -3.8343e-09, -6.0820e-09,\n",
      "        -9.2553e-10,  2.7105e-09,  2.6576e-08,  3.8013e-09,  7.9397e-08,\n",
      "        -1.4147e-08,  3.4245e-08,  3.7748e-08, -7.8670e-09,  2.3932e-08,\n",
      "         3.3583e-08, -2.2907e-08, -4.2310e-09,  2.1419e-08,  3.2790e-08,\n",
      "         5.9498e-09,  3.0741e-09, -7.5364e-09,  1.3090e-08,  2.4857e-08,\n",
      "        -1.4412e-08, -1.5866e-09, -8.2636e-09,  3.7947e-08, -4.2310e-08,\n",
      "        -3.8145e-08,  8.2636e-09,  6.5779e-09,  2.9815e-08,  3.6691e-08,\n",
      "         3.2922e-08,  5.3945e-08, -4.4128e-09, -1.5337e-08, -2.0097e-08,\n",
      "        -3.7484e-08,  3.4112e-08, -2.3865e-08, -3.3716e-08,  4.2442e-08,\n",
      "        -7.9331e-10,  7.0076e-09,  2.4659e-08, -2.1750e-08, -3.0080e-08,\n",
      "         7.3613e-08, -2.2477e-09,  3.7616e-08,  1.1767e-08, -4.0062e-08,\n",
      "         1.1503e-08, -1.9965e-08,  2.0163e-08, -2.7369e-08, -5.6193e-09,\n",
      "        -1.3817e-08, -3.4377e-09, -1.3106e-08, -7.2390e-09, -8.8586e-09,\n",
      "         1.8643e-08,  1.7056e-08, -2.5121e-09, -1.9172e-09, -2.9088e-08,\n",
      "        -1.4676e-08, -1.5866e-09, -2.5849e-08,  2.6245e-08, -1.9833e-10,\n",
      "         8.0918e-08, -1.8577e-08, -2.7948e-08, -3.5038e-09, -3.3583e-08,\n",
      "         1.5403e-08, -2.2659e-08, -6.7431e-09, -1.3585e-08,  1.3420e-08,\n",
      "         2.7634e-08,  1.6478e-08, -2.0494e-08, -1.9436e-08, -8.5942e-09,\n",
      "         3.2129e-08, -2.1221e-08, -4.9251e-08,  4.9846e-08, -1.1635e-08,\n",
      "         4.7070e-08, -1.1048e-08,  6.6770e-09,  2.8956e-08,  2.3006e-08,\n",
      "        -4.7516e-08,  3.0179e-08,  2.3799e-08,  3.8839e-08,  2.3601e-08,\n",
      "        -1.5602e-08,  2.0494e-09, -1.4544e-08,  2.6840e-08, -3.6757e-08,\n",
      "         1.9833e-09,  2.1816e-09,  1.3883e-08,  1.8577e-08, -1.3883e-09,\n",
      "         4.3566e-08,  5.9300e-08, -3.6261e-08, -2.3403e-08,  3.8575e-08,\n",
      "        -1.2296e-08,  8.7925e-09,  1.4875e-08,  7.1398e-09, -1.9833e-09,\n",
      "         3.7352e-09, -3.7682e-08,  1.3057e-09,  2.3998e-08,  5.5399e-08,\n",
      "        -3.0675e-08, -7.6356e-09, -2.4460e-09,  3.0939e-08, -1.0049e-08,\n",
      "        -3.2261e-08, -7.1398e-09,  4.5615e-09, -5.1830e-08, -6.6109e-09,\n",
      "         5.0904e-09,  2.2477e-09, -2.0626e-08,  1.8709e-08,  1.5602e-08,\n",
      "         9.7180e-09, -6.4787e-09, -1.7849e-09, -3.3385e-08, -1.5602e-08,\n",
      "        -2.7039e-08,  1.9172e-08, -3.0741e-08, -2.2477e-09,  1.9039e-08,\n",
      "        -5.5119e-08,  7.1728e-09, -2.6973e-08,  1.2296e-08,  1.0049e-08,\n",
      "         3.7054e-08, -3.4245e-08,  8.5942e-10,  2.1010e-08,  5.3879e-09,\n",
      "        -2.4196e-08,  6.1812e-08, -4.5615e-09, -1.8180e-08,  4.0789e-08,\n",
      "        -2.2180e-08,  2.9088e-09,  2.1552e-08,  6.9580e-09, -1.0577e-08,\n",
      "        -4.5714e-08,  5.0970e-08,  1.5139e-08,  2.9881e-08,  1.0049e-08,\n",
      "        -7.9331e-10,  5.4375e-08, -1.2462e-08, -4.2310e-09,  3.8740e-08,\n",
      "        -4.2971e-09, -6.4787e-09, -4.0194e-08,  5.1565e-09, -4.4954e-08,\n",
      "        -1.6065e-08, -2.0229e-08,  1.4676e-08,  2.4989e-08,  6.1614e-08,\n",
      "        -4.0657e-09,  6.0820e-09, -9.3214e-09,  3.5831e-08, -4.4558e-08,\n",
      "         8.7264e-09, -2.2708e-08,  4.3632e-09,  2.5518e-08,  1.7056e-08])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T00:22:44.409670Z",
     "start_time": "2025-11-27T00:22:44.405977Z"
    }
   },
   "cell_type": "code",
   "source": "print(y_tensor)",
   "id": "47dfd2302b0b0e2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([43, 16, 21,  ..., 32, 46, 33])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T00:22:44.785395Z",
     "start_time": "2025-11-27T00:22:44.774801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# total samples\n",
    "n_samples = len(X_tensor)\n",
    "n_train = int(0.8 * n_samples)\n",
    "n_val = n_samples - n_train\n",
    "\n",
    "# combine into a dataset\n",
    "dataset = TensorDataset(X_tensor_norm, y_tensor)\n",
    "\n",
    "# random split\n",
    "utils.set_seed(433)\n",
    "g = torch.Generator().manual_seed(433)\n",
    "train_set, val_set = random_split(dataset, [n_train, n_val], generator=g)\n",
    "\n",
    "print(f\"Train samples: {len(train_set)}\")\n",
    "print(f\"Validation samples: {len(val_set)}\")"
   ],
   "id": "9c85f7dda248de9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 92324\n",
      "Validation samples: 23082\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Performing K-Fold Cross Validation",
   "id": "f8fe6a082c45cbaf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T00:22:46.644994Z",
     "start_time": "2025-11-27T00:22:46.637618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def k_fold_cross_validation(X_tensor, y_tensor, device, params, k=5, grad_clip=True, gauss=True, log=True, patience=5):\n",
    "    \"\"\"\n",
    "    Perform K-fold cross-validation using PyTorch only.\n",
    "\n",
    "    Args:\n",
    "        X_tensor: training dataset features\n",
    "        y_tensor: training dataset labels\n",
    "        device: device to run training on (\"cpu\" or \"cuda\")\n",
    "        params: dictionary of training hyperparameters\n",
    "        k: number of folds\n",
    "        grad_clip: whether to apply gradient clipping during training\n",
    "        gauss: whether to apply Gaussian noise\n",
    "        log: whether to log training and validation metrics during training\n",
    "        patience: number of epochs to wait for validation improvement before early stopping\n",
    "\n",
    "    Returns:\n",
    "        model: the trained model with the best validation weights restored\n",
    "        fold_train_accs: list of training accuracies for each fold\n",
    "        fold_val_accs: list validation accuracies for each fold\n",
    "        fold_losses: list of training losses for each fold\n",
    "        fold_best_val_accs: list of the best validation accuracy for each fold\n",
    "    \"\"\"\n",
    "    utils.set_seed(433)\n",
    "\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    n_samples = len(dataset)\n",
    "    indices = torch.randperm(n_samples)  # shuffle indices reproducibly\n",
    "\n",
    "    fold_size = n_samples // k\n",
    "\n",
    "    models = []\n",
    "    fold_train_accs = []\n",
    "    fold_val_accs = []\n",
    "    fold_best_val_accs = []\n",
    "    fold_losses = []\n",
    "\n",
    "    for fold in range(k):\n",
    "        print(f\"\\n===== Fold {fold+1}/{k} =====\")\n",
    "\n",
    "        # determine validation indices\n",
    "        start = fold * fold_size\n",
    "        end = start + fold_size if fold != k - 1 else n_samples  # last fold takes the remainder\n",
    "        val_idx = indices[start:end]\n",
    "        train_idx = torch.cat([indices[:start], indices[end:]])\n",
    "\n",
    "        # create subsets\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        # train the model on this fold\n",
    "        model, train_accs, val_accs, losses, best_val_acc = utils.train_and_validate(\n",
    "            train_subset, val_subset, device, params=params, grad_clip=grad_clip, gauss=gauss, log=log, patience=patience\n",
    "        )\n",
    "\n",
    "        models.append(model)\n",
    "        fold_train_accs.append(train_accs)\n",
    "        fold_val_accs.append(val_accs)\n",
    "        fold_best_val_accs.append(best_val_acc)\n",
    "        fold_losses.append(losses)\n",
    "\n",
    "        print(f\"Fold {fold+1} Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "    # wait this doesn't help at all if we just take fold_val_accs... should take fold_best_val_accs instead.\n",
    "    mean_acc = float(torch.tensor(fold_best_val_accs).mean())\n",
    "    std_acc = float(torch.tensor(fold_best_val_accs).std())\n",
    "\n",
    "    print(f\"\\nK-Fold Validation Results ({k} folds): Mean = {mean_acc:.2f}%, Std = {std_acc:.2f}%\")\n",
    "\n",
    "    return models, fold_train_accs, fold_val_accs, fold_best_val_accs, fold_losses"
   ],
   "id": "80b85fc33831f23e",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T01:14:39.975263Z",
     "start_time": "2025-11-27T00:22:47.270268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "\n",
    "    This cell will run  k-fold validation, which will take a very long time.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Medium MLP first submission w/modified architecture, gets 0.827 on FULL TRAIN SET\n",
    "# grad_clip=True, gauss=True, patience=10. HITS 81.05% VAL ACC\n",
    "params = {'hidden_size': 4096, 'lr': 0.001, 'weight_decay': 0.1, 'batch_size': 512, 'init_type': 'xavier', 'dropout': 0.5, 'noise_std': 0.65, 'num_epochs': 100, 'warmup_epochs': 12}\n",
    "\n",
    "# Medium MLP second submission w/modified architecture, gets 0.834 on FULL TRAIN SET\n",
    "# grad_clip=True, gauss=True, patience=10. HITS 81.45% VAL ACC\n",
    "params = {'hidden_size': 4096, 'lr': 0.005, 'weight_decay': 0.05, 'batch_size': 512, 'init_type': 'xavier', 'dropout': 0.5, 'noise_std': 0.6, 'num_epochs': 100, 'warmup_epochs': 12}\n",
    "\n",
    "# Medium MLP third submission w/modified architecture, gets 0.839 on FULL TRAIN SET\n",
    "# grad_clip=True, gauss=True, patience=10. HITS 81.93% VAL ACC\n",
    "# grad_clip=True, gauss=True, patience=10, label_smoothing=0.15, max_norm=10.0. HITS 82.65% VAL ACC\n",
    "params = {'hidden_size': 4096, 'lr': 0.005, 'weight_decay': 0.05, 'batch_size': 512, 'init_type': 'xavier', 'dropout': 0.5, 'noise_std': 0.6, 'num_epochs': 125, 'warmup_epochs': 12}\n",
    "\n",
    "models, fold_train_accs, fold_val_accs, fold_best_val_accs, fold_losses = k_fold_cross_validation(\n",
    "    X_tensor_norm, y_tensor, device, params, k=5, grad_clip=True, gauss=True, log=True, patience=10\n",
    ")"
   ],
   "id": "82284eaf64730172",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1/5 =====\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41785c3069b54061b838d779d486e31f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/125] | Train Acc: 1.96% | Val Acc: 1.99% | Loss: 4.3651023191673595\n",
      "Epoch [2/125] | Train Acc: 21.35% | Val Acc: 39.58% | Loss: 3.3904668094703183\n",
      "Epoch [3/125] | Train Acc: 34.54% | Val Acc: 47.58% | Loss: 2.9099782410153456\n",
      "Epoch [4/125] | Train Acc: 41.69% | Val Acc: 52.44% | Loss: 2.687107433773075\n",
      "Epoch [5/125] | Train Acc: 46.51% | Val Acc: 54.91% | Loss: 2.5493847274057204\n",
      "Epoch [6/125] | Train Acc: 49.63% | Val Acc: 57.04% | Loss: 2.4488761279087807\n",
      "Epoch [7/125] | Train Acc: 52.85% | Val Acc: 59.27% | Loss: 2.360531159312364\n",
      "Epoch [8/125] | Train Acc: 55.68% | Val Acc: 61.44% | Loss: 2.2792690683334396\n",
      "Epoch [9/125] | Train Acc: 58.73% | Val Acc: 62.26% | Loss: 2.1949017452567436\n",
      "Epoch [10/125] | Train Acc: 61.78% | Val Acc: 63.89% | Loss: 2.111141539831133\n",
      "Epoch [11/125] | Train Acc: 64.37% | Val Acc: 65.91% | Loss: 2.0403343357950763\n",
      "Epoch [12/125] | Train Acc: 66.48% | Val Acc: 66.19% | Loss: 1.9844195529370652\n",
      "Epoch [13/125] | Train Acc: 68.32% | Val Acc: 66.49% | Loss: 1.9383066123602934\n",
      "Epoch [14/125] | Train Acc: 70.77% | Val Acc: 68.40% | Loss: 1.8774214442377326\n",
      "Epoch [15/125] | Train Acc: 72.81% | Val Acc: 69.33% | Loss: 1.8296360804671168\n",
      "Epoch [16/125] | Train Acc: 74.88% | Val Acc: 69.63% | Loss: 1.7794860431672175\n",
      "Epoch [17/125] | Train Acc: 75.74% | Val Acc: 71.03% | Loss: 1.7521742955591826\n",
      "Epoch [18/125] | Train Acc: 76.97% | Val Acc: 70.98% | Loss: 1.7227636492998013\n",
      "Epoch [19/125] | Train Acc: 78.20% | Val Acc: 71.52% | Loss: 1.696309068916356\n",
      "Epoch [20/125] | Train Acc: 78.78% | Val Acc: 71.31% | Loss: 1.6808032560516366\n",
      "Epoch [21/125] | Train Acc: 79.61% | Val Acc: 72.09% | Loss: 1.6640562421791991\n",
      "Epoch [22/125] | Train Acc: 80.30% | Val Acc: 72.08% | Loss: 1.647227774180276\n",
      "Epoch [23/125] | Train Acc: 80.86% | Val Acc: 72.03% | Loss: 1.6344367716650055\n",
      "Epoch [24/125] | Train Acc: 81.11% | Val Acc: 72.43% | Loss: 1.624985017201909\n",
      "Epoch [25/125] | Train Acc: 81.63% | Val Acc: 73.30% | Loss: 1.6151558646670274\n",
      "Epoch [26/125] | Train Acc: 82.12% | Val Acc: 72.94% | Loss: 1.6057703556667136\n",
      "Epoch [27/125] | Train Acc: 82.38% | Val Acc: 72.76% | Loss: 1.60082032182757\n",
      "Epoch [28/125] | Train Acc: 82.86% | Val Acc: 73.25% | Loss: 1.5893678594894471\n",
      "Epoch [29/125] | Train Acc: 83.02% | Val Acc: 73.32% | Loss: 1.5820496223252414\n",
      "Epoch [30/125] | Train Acc: 83.33% | Val Acc: 73.61% | Loss: 1.5786180811473227\n",
      "Epoch [31/125] | Train Acc: 83.50% | Val Acc: 73.63% | Loss: 1.5725051315761083\n",
      "Epoch [32/125] | Train Acc: 84.06% | Val Acc: 73.49% | Loss: 1.5602149840005524\n",
      "Epoch [33/125] | Train Acc: 84.50% | Val Acc: 73.89% | Loss: 1.5521560503956815\n",
      "Epoch [34/125] | Train Acc: 84.52% | Val Acc: 74.49% | Loss: 1.5499013926683969\n",
      "Epoch [35/125] | Train Acc: 84.67% | Val Acc: 74.55% | Loss: 1.5447466832875496\n",
      "Epoch [36/125] | Train Acc: 85.10% | Val Acc: 74.28% | Loss: 1.5359112155556904\n",
      "Epoch [37/125] | Train Acc: 85.33% | Val Acc: 74.48% | Loss: 1.5304439123542593\n",
      "Epoch [38/125] | Train Acc: 85.95% | Val Acc: 74.81% | Loss: 1.5189482381600143\n",
      "Epoch [39/125] | Train Acc: 85.99% | Val Acc: 74.83% | Loss: 1.5151567354649023\n",
      "Epoch [40/125] | Train Acc: 86.37% | Val Acc: 74.85% | Loss: 1.5082922980212727\n",
      "Epoch [41/125] | Train Acc: 86.41% | Val Acc: 74.89% | Loss: 1.5060751992617394\n",
      "Epoch [42/125] | Train Acc: 86.66% | Val Acc: 75.17% | Loss: 1.501178809866297\n",
      "Epoch [43/125] | Train Acc: 87.10% | Val Acc: 75.02% | Loss: 1.4887186188262684\n",
      "Epoch [44/125] | Train Acc: 87.49% | Val Acc: 75.24% | Loss: 1.4809312099794563\n",
      "Epoch [45/125] | Train Acc: 88.02% | Val Acc: 74.88% | Loss: 1.472195355132315\n",
      "Epoch [46/125] | Train Acc: 87.95% | Val Acc: 75.21% | Loss: 1.4714256602782336\n",
      "Epoch [47/125] | Train Acc: 87.98% | Val Acc: 75.77% | Loss: 1.4659720104108036\n",
      "Epoch [48/125] | Train Acc: 88.38% | Val Acc: 75.81% | Loss: 1.460113424044975\n",
      "Epoch [49/125] | Train Acc: 88.94% | Val Acc: 75.88% | Loss: 1.4510150806490012\n",
      "Epoch [50/125] | Train Acc: 89.18% | Val Acc: 75.69% | Loss: 1.445150622283415\n",
      "Epoch [51/125] | Train Acc: 89.36% | Val Acc: 75.79% | Loss: 1.4411249908289223\n",
      "Epoch [52/125] | Train Acc: 89.59% | Val Acc: 76.23% | Loss: 1.4344640912216853\n",
      "Epoch [53/125] | Train Acc: 89.83% | Val Acc: 76.34% | Loss: 1.4295346176040034\n",
      "Epoch [54/125] | Train Acc: 90.00% | Val Acc: 76.59% | Loss: 1.4237973011124272\n",
      "Epoch [55/125] | Train Acc: 90.30% | Val Acc: 76.58% | Loss: 1.4178379805766566\n",
      "Epoch [56/125] | Train Acc: 90.38% | Val Acc: 76.92% | Loss: 1.4150350205426354\n",
      "Epoch [57/125] | Train Acc: 90.83% | Val Acc: 76.82% | Loss: 1.4061995919390549\n",
      "Epoch [58/125] | Train Acc: 91.08% | Val Acc: 76.88% | Loss: 1.3994361166963079\n",
      "Epoch [59/125] | Train Acc: 91.11% | Val Acc: 77.47% | Loss: 1.396733526269369\n",
      "Epoch [60/125] | Train Acc: 91.42% | Val Acc: 77.54% | Loss: 1.3901088654029798\n",
      "Epoch [61/125] | Train Acc: 91.64% | Val Acc: 76.90% | Loss: 1.3846596037453829\n",
      "Epoch [62/125] | Train Acc: 91.94% | Val Acc: 77.18% | Loss: 1.3769148629434123\n",
      "Epoch [63/125] | Train Acc: 92.34% | Val Acc: 77.51% | Loss: 1.3700125528563338\n",
      "Epoch [64/125] | Train Acc: 92.44% | Val Acc: 77.76% | Loss: 1.3678106142465511\n",
      "Epoch [65/125] | Train Acc: 92.69% | Val Acc: 77.64% | Loss: 1.3596940364303094\n",
      "Epoch [66/125] | Train Acc: 92.86% | Val Acc: 77.93% | Loss: 1.35675870295498\n",
      "Epoch [67/125] | Train Acc: 93.04% | Val Acc: 78.05% | Loss: 1.352192617220637\n",
      "Epoch [68/125] | Train Acc: 93.22% | Val Acc: 78.16% | Loss: 1.3452956546976609\n",
      "Epoch [69/125] | Train Acc: 93.43% | Val Acc: 78.17% | Loss: 1.3413309447784072\n",
      "Epoch [70/125] | Train Acc: 93.70% | Val Acc: 78.23% | Loss: 1.3341355989116852\n",
      "Epoch [71/125] | Train Acc: 93.75% | Val Acc: 78.51% | Loss: 1.332800302734034\n",
      "Epoch [72/125] | Train Acc: 94.10% | Val Acc: 78.69% | Loss: 1.324333303113504\n",
      "Epoch [73/125] | Train Acc: 94.33% | Val Acc: 78.88% | Loss: 1.3188329671786918\n",
      "Epoch [74/125] | Train Acc: 94.52% | Val Acc: 79.00% | Loss: 1.3142875888939638\n",
      "Epoch [75/125] | Train Acc: 94.79% | Val Acc: 79.00% | Loss: 1.3086575639264033\n",
      "Epoch [76/125] | Train Acc: 94.78% | Val Acc: 78.80% | Loss: 1.304990024207699\n",
      "Epoch [77/125] | Train Acc: 95.08% | Val Acc: 78.96% | Loss: 1.3001446308122557\n",
      "Epoch [78/125] | Train Acc: 95.39% | Val Acc: 79.21% | Loss: 1.29366089461394\n",
      "Epoch [79/125] | Train Acc: 95.23% | Val Acc: 79.50% | Loss: 1.2944750546673078\n",
      "Epoch [80/125] | Train Acc: 95.49% | Val Acc: 79.39% | Loss: 1.2884527741658665\n",
      "Epoch [81/125] | Train Acc: 95.82% | Val Acc: 79.58% | Loss: 1.2797042843167645\n",
      "Epoch [82/125] | Train Acc: 95.78% | Val Acc: 79.81% | Loss: 1.2786351087375478\n",
      "Epoch [83/125] | Train Acc: 96.11% | Val Acc: 79.58% | Loss: 1.2700352821019552\n",
      "Epoch [84/125] | Train Acc: 96.16% | Val Acc: 79.60% | Loss: 1.2682391971770968\n",
      "Epoch [85/125] | Train Acc: 96.26% | Val Acc: 80.08% | Loss: 1.2661676574534528\n",
      "Epoch [86/125] | Train Acc: 96.46% | Val Acc: 79.76% | Loss: 1.2592224454298724\n",
      "Epoch [87/125] | Train Acc: 96.61% | Val Acc: 80.19% | Loss: 1.2560916603970522\n",
      "Epoch [88/125] | Train Acc: 96.69% | Val Acc: 80.28% | Loss: 1.2536226883032702\n",
      "Epoch [89/125] | Train Acc: 96.94% | Val Acc: 80.55% | Loss: 1.248536895848276\n",
      "Epoch [90/125] | Train Acc: 96.98% | Val Acc: 80.46% | Loss: 1.2423475543845162\n",
      "Epoch [91/125] | Train Acc: 97.11% | Val Acc: 80.61% | Loss: 1.2393140459112317\n",
      "Epoch [92/125] | Train Acc: 97.27% | Val Acc: 80.69% | Loss: 1.2352669390604292\n",
      "Epoch [93/125] | Train Acc: 97.34% | Val Acc: 80.62% | Loss: 1.2324784516423108\n",
      "Epoch [94/125] | Train Acc: 97.38% | Val Acc: 80.84% | Loss: 1.2293973296281933\n",
      "Epoch [95/125] | Train Acc: 97.56% | Val Acc: 80.88% | Loss: 1.2263877604603606\n",
      "Epoch [96/125] | Train Acc: 97.68% | Val Acc: 80.94% | Loss: 1.222325331794838\n",
      "Epoch [97/125] | Train Acc: 97.73% | Val Acc: 81.07% | Loss: 1.2197456449549469\n",
      "Epoch [98/125] | Train Acc: 97.79% | Val Acc: 80.84% | Loss: 1.2161184504307805\n",
      "Epoch [99/125] | Train Acc: 97.90% | Val Acc: 80.97% | Loss: 1.2130450096590035\n",
      "Epoch [100/125] | Train Acc: 97.95% | Val Acc: 81.34% | Loss: 1.2098580707007232\n",
      "Epoch [101/125] | Train Acc: 98.03% | Val Acc: 81.28% | Loss: 1.2086757880873775\n",
      "Epoch [102/125] | Train Acc: 98.14% | Val Acc: 81.34% | Loss: 1.205644345543038\n",
      "Epoch [103/125] | Train Acc: 98.20% | Val Acc: 81.39% | Loss: 1.2033425069468602\n",
      "Epoch [104/125] | Train Acc: 98.31% | Val Acc: 81.60% | Loss: 1.2002283724904415\n",
      "Epoch [105/125] | Train Acc: 98.30% | Val Acc: 81.61% | Loss: 1.1982069203138932\n",
      "Epoch [106/125] | Train Acc: 98.35% | Val Acc: 81.44% | Loss: 1.1957754264896618\n",
      "Epoch [107/125] | Train Acc: 98.43% | Val Acc: 81.65% | Loss: 1.194746292510912\n",
      "Epoch [108/125] | Train Acc: 98.47% | Val Acc: 81.74% | Loss: 1.1930441138546157\n",
      "Epoch [109/125] | Train Acc: 98.48% | Val Acc: 81.71% | Loss: 1.191190079268872\n",
      "Epoch [110/125] | Train Acc: 98.54% | Val Acc: 81.81% | Loss: 1.1892694046114507\n",
      "Epoch [111/125] | Train Acc: 98.56% | Val Acc: 81.93% | Loss: 1.1886438992324961\n",
      "Epoch [112/125] | Train Acc: 98.60% | Val Acc: 81.82% | Loss: 1.1867166433210397\n",
      "Epoch [113/125] | Train Acc: 98.68% | Val Acc: 82.01% | Loss: 1.1863788636796622\n",
      "Epoch [114/125] | Train Acc: 98.70% | Val Acc: 81.99% | Loss: 1.1839603517890267\n",
      "Epoch [115/125] | Train Acc: 98.63% | Val Acc: 81.97% | Loss: 1.1852989335076904\n",
      "Epoch [116/125] | Train Acc: 98.72% | Val Acc: 81.92% | Loss: 1.1822385476993216\n",
      "Epoch [117/125] | Train Acc: 98.62% | Val Acc: 81.90% | Loss: 1.1839200043516884\n",
      "Epoch [118/125] | Train Acc: 98.70% | Val Acc: 81.96% | Loss: 1.1823300274490762\n",
      "Epoch [119/125] | Train Acc: 98.71% | Val Acc: 81.96% | Loss: 1.1807379578911383\n",
      "Epoch [120/125] | Train Acc: 98.77% | Val Acc: 82.03% | Loss: 1.1808683415192625\n",
      "Epoch [121/125] | Train Acc: 98.68% | Val Acc: 82.02% | Loss: 1.1812013686358558\n",
      "Epoch [122/125] | Train Acc: 98.77% | Val Acc: 81.99% | Loss: 1.1803504047347315\n",
      "Epoch [123/125] | Train Acc: 98.74% | Val Acc: 82.01% | Loss: 1.179179246517949\n",
      "Epoch [124/125] | Train Acc: 98.68% | Val Acc: 82.10% | Loss: 1.180923313713642\n",
      "Epoch [125/125] | Train Acc: 98.73% | Val Acc: 81.96% | Loss: 1.1813862749376562\n",
      "Fold 1 Best Validation Accuracy: 82.10%\n",
      "\n",
      "===== Fold 2/5 =====\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50c8d83b40b745559851ba7e607c6e4e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/125] | Train Acc: 1.98% | Val Acc: 1.92% | Loss: 4.363386110811417\n",
      "Epoch [2/125] | Train Acc: 21.14% | Val Acc: 39.83% | Loss: 3.3906664583788246\n",
      "Epoch [3/125] | Train Acc: 34.32% | Val Acc: 47.57% | Loss: 2.9147979160353255\n",
      "Epoch [4/125] | Train Acc: 41.69% | Val Acc: 52.27% | Loss: 2.685438067071844\n",
      "Epoch [5/125] | Train Acc: 46.33% | Val Acc: 54.90% | Loss: 2.547281445405879\n",
      "Epoch [6/125] | Train Acc: 49.94% | Val Acc: 56.90% | Loss: 2.443604927207602\n",
      "Epoch [7/125] | Train Acc: 52.98% | Val Acc: 58.84% | Loss: 2.355111506991744\n",
      "Epoch [8/125] | Train Acc: 55.88% | Val Acc: 60.87% | Loss: 2.2748509401981987\n",
      "Epoch [9/125] | Train Acc: 58.71% | Val Acc: 62.28% | Loss: 2.191845334236647\n",
      "Epoch [10/125] | Train Acc: 61.84% | Val Acc: 63.69% | Loss: 2.111089475617769\n",
      "Epoch [11/125] | Train Acc: 64.46% | Val Acc: 64.74% | Loss: 2.04173057208047\n",
      "Epoch [12/125] | Train Acc: 66.67% | Val Acc: 65.65% | Loss: 1.9813848089067845\n",
      "Epoch [13/125] | Train Acc: 68.73% | Val Acc: 66.06% | Loss: 1.9296561542978914\n",
      "Epoch [14/125] | Train Acc: 70.79% | Val Acc: 67.79% | Loss: 1.876205084581476\n",
      "Epoch [15/125] | Train Acc: 72.94% | Val Acc: 68.57% | Loss: 1.8225623637642798\n",
      "Epoch [16/125] | Train Acc: 74.74% | Val Acc: 69.15% | Loss: 1.780851730977556\n",
      "Epoch [17/125] | Train Acc: 76.16% | Val Acc: 69.26% | Loss: 1.7437789997175204\n",
      "Epoch [18/125] | Train Acc: 77.31% | Val Acc: 70.86% | Loss: 1.7182286200509174\n",
      "Epoch [19/125] | Train Acc: 78.17% | Val Acc: 70.63% | Loss: 1.6959953334716933\n",
      "Epoch [20/125] | Train Acc: 78.96% | Val Acc: 70.63% | Loss: 1.6783004316326857\n",
      "Epoch [21/125] | Train Acc: 79.50% | Val Acc: 71.39% | Loss: 1.6642005351924198\n",
      "Epoch [22/125] | Train Acc: 80.20% | Val Acc: 71.88% | Loss: 1.6473523963398318\n",
      "Epoch [23/125] | Train Acc: 80.86% | Val Acc: 71.49% | Loss: 1.6339524430348045\n",
      "Epoch [24/125] | Train Acc: 81.29% | Val Acc: 72.12% | Loss: 1.6247422078021845\n",
      "Epoch [25/125] | Train Acc: 81.69% | Val Acc: 72.31% | Loss: 1.6134119665877722\n",
      "Epoch [26/125] | Train Acc: 81.95% | Val Acc: 72.80% | Loss: 1.607513800398173\n",
      "Epoch [27/125] | Train Acc: 82.35% | Val Acc: 72.35% | Loss: 1.5992284614956545\n",
      "Epoch [28/125] | Train Acc: 82.86% | Val Acc: 72.60% | Loss: 1.5879972254690877\n",
      "Epoch [29/125] | Train Acc: 83.13% | Val Acc: 72.87% | Loss: 1.5807651142411654\n",
      "Epoch [30/125] | Train Acc: 83.16% | Val Acc: 72.89% | Loss: 1.5810946020265275\n",
      "Epoch [31/125] | Train Acc: 83.76% | Val Acc: 73.08% | Loss: 1.5690418940367752\n",
      "Epoch [32/125] | Train Acc: 84.12% | Val Acc: 73.29% | Loss: 1.5614038565899342\n",
      "Epoch [33/125] | Train Acc: 84.39% | Val Acc: 72.87% | Loss: 1.5547504337957936\n",
      "Epoch [34/125] | Train Acc: 84.64% | Val Acc: 73.36% | Loss: 1.5468190662472867\n",
      "Epoch [35/125] | Train Acc: 84.76% | Val Acc: 73.66% | Loss: 1.5434676618780059\n",
      "Epoch [36/125] | Train Acc: 85.32% | Val Acc: 73.48% | Loss: 1.5322772481284619\n",
      "Epoch [37/125] | Train Acc: 85.39% | Val Acc: 74.04% | Loss: 1.5291386482510954\n",
      "Epoch [38/125] | Train Acc: 85.86% | Val Acc: 74.05% | Loss: 1.519122933824954\n",
      "Epoch [39/125] | Train Acc: 85.87% | Val Acc: 74.22% | Loss: 1.5166291146055004\n",
      "Epoch [40/125] | Train Acc: 86.28% | Val Acc: 74.21% | Loss: 1.5087484970474967\n",
      "Epoch [41/125] | Train Acc: 86.46% | Val Acc: 74.41% | Loss: 1.5039425017224912\n",
      "Epoch [42/125] | Train Acc: 86.91% | Val Acc: 74.68% | Loss: 1.4952690610671282\n",
      "Epoch [43/125] | Train Acc: 87.38% | Val Acc: 74.55% | Loss: 1.4860024581315747\n",
      "Epoch [44/125] | Train Acc: 87.46% | Val Acc: 74.57% | Loss: 1.4841268793173739\n",
      "Epoch [45/125] | Train Acc: 87.73% | Val Acc: 74.73% | Loss: 1.4753892300879765\n",
      "Epoch [46/125] | Train Acc: 88.11% | Val Acc: 75.04% | Loss: 1.4702234173673232\n",
      "Epoch [47/125] | Train Acc: 88.34% | Val Acc: 75.71% | Loss: 1.4636934790661624\n",
      "Epoch [48/125] | Train Acc: 88.70% | Val Acc: 75.23% | Loss: 1.4564845602860337\n",
      "Epoch [49/125] | Train Acc: 88.61% | Val Acc: 75.79% | Loss: 1.456291108496816\n",
      "Epoch [50/125] | Train Acc: 88.93% | Val Acc: 75.54% | Loss: 1.4508543467695978\n",
      "Epoch [51/125] | Train Acc: 89.67% | Val Acc: 75.91% | Loss: 1.4357961563892296\n",
      "Epoch [52/125] | Train Acc: 89.49% | Val Acc: 75.86% | Loss: 1.436418033579711\n",
      "Epoch [53/125] | Train Acc: 89.93% | Val Acc: 76.02% | Loss: 1.427920626448707\n",
      "Epoch [54/125] | Train Acc: 90.22% | Val Acc: 76.37% | Loss: 1.4227961745263438\n",
      "Epoch [55/125] | Train Acc: 90.49% | Val Acc: 76.39% | Loss: 1.4149351387207274\n",
      "Epoch [56/125] | Train Acc: 90.69% | Val Acc: 76.43% | Loss: 1.4100371966154117\n",
      "Epoch [57/125] | Train Acc: 91.05% | Val Acc: 76.73% | Loss: 1.402111061374056\n",
      "Epoch [58/125] | Train Acc: 91.14% | Val Acc: 76.45% | Loss: 1.3985400985141385\n",
      "Epoch [59/125] | Train Acc: 91.33% | Val Acc: 76.50% | Loss: 1.3930571587635125\n",
      "Epoch [60/125] | Train Acc: 91.68% | Val Acc: 77.18% | Loss: 1.3845295200804981\n",
      "Epoch [61/125] | Train Acc: 91.72% | Val Acc: 76.65% | Loss: 1.3841831955237018\n",
      "Epoch [62/125] | Train Acc: 92.12% | Val Acc: 77.05% | Loss: 1.374461912862208\n",
      "Epoch [63/125] | Train Acc: 92.16% | Val Acc: 76.98% | Loss: 1.3727861300297233\n",
      "Epoch [64/125] | Train Acc: 92.55% | Val Acc: 77.39% | Loss: 1.3636201728277983\n",
      "Epoch [65/125] | Train Acc: 92.58% | Val Acc: 77.60% | Loss: 1.3610681188251796\n",
      "Epoch [66/125] | Train Acc: 92.91% | Val Acc: 77.60% | Loss: 1.3553372726471418\n",
      "Epoch [67/125] | Train Acc: 93.10% | Val Acc: 77.58% | Loss: 1.351089041635269\n",
      "Epoch [68/125] | Train Acc: 93.31% | Val Acc: 77.86% | Loss: 1.345137549629387\n",
      "Epoch [69/125] | Train Acc: 93.59% | Val Acc: 77.89% | Loss: 1.3376234533119873\n",
      "Epoch [70/125] | Train Acc: 93.71% | Val Acc: 78.25% | Loss: 1.3338905311786415\n",
      "Epoch [71/125] | Train Acc: 94.03% | Val Acc: 78.50% | Loss: 1.3279473546950307\n",
      "Epoch [72/125] | Train Acc: 94.17% | Val Acc: 78.32% | Loss: 1.323987757083429\n",
      "Epoch [73/125] | Train Acc: 94.30% | Val Acc: 78.61% | Loss: 1.319669097663586\n",
      "Epoch [74/125] | Train Acc: 94.64% | Val Acc: 78.30% | Loss: 1.3125461784378805\n",
      "Epoch [75/125] | Train Acc: 94.66% | Val Acc: 78.79% | Loss: 1.3099421164248972\n",
      "Epoch [76/125] | Train Acc: 94.92% | Val Acc: 78.90% | Loss: 1.3020583319050685\n",
      "Epoch [77/125] | Train Acc: 95.10% | Val Acc: 78.90% | Loss: 1.2991768290000396\n",
      "Epoch [78/125] | Train Acc: 95.30% | Val Acc: 78.76% | Loss: 1.2918418383139953\n",
      "Epoch [79/125] | Train Acc: 95.41% | Val Acc: 79.44% | Loss: 1.2907345464873243\n",
      "Epoch [80/125] | Train Acc: 95.58% | Val Acc: 79.16% | Loss: 1.2857314376949844\n",
      "Epoch [81/125] | Train Acc: 95.67% | Val Acc: 79.56% | Loss: 1.2822967703465746\n",
      "Epoch [82/125] | Train Acc: 95.82% | Val Acc: 79.63% | Loss: 1.275966131903439\n",
      "Epoch [83/125] | Train Acc: 96.03% | Val Acc: 79.64% | Loss: 1.272729679904848\n",
      "Epoch [84/125] | Train Acc: 96.18% | Val Acc: 79.55% | Loss: 1.2670615052131013\n",
      "Epoch [85/125] | Train Acc: 96.41% | Val Acc: 79.80% | Loss: 1.263429252888948\n",
      "Epoch [86/125] | Train Acc: 96.43% | Val Acc: 79.87% | Loss: 1.259639422943235\n",
      "Epoch [87/125] | Train Acc: 96.71% | Val Acc: 79.99% | Loss: 1.2550049398741423\n",
      "Epoch [88/125] | Train Acc: 96.88% | Val Acc: 80.16% | Loss: 1.2503880268495127\n",
      "Epoch [89/125] | Train Acc: 96.85% | Val Acc: 80.33% | Loss: 1.2464360136731414\n",
      "Epoch [90/125] | Train Acc: 96.95% | Val Acc: 80.45% | Loss: 1.2438930011137759\n",
      "Epoch [91/125] | Train Acc: 97.23% | Val Acc: 80.37% | Loss: 1.23768073991744\n",
      "Epoch [92/125] | Train Acc: 97.33% | Val Acc: 80.57% | Loss: 1.2332723258111717\n",
      "Epoch [93/125] | Train Acc: 97.53% | Val Acc: 80.60% | Loss: 1.2307358634488033\n",
      "Epoch [94/125] | Train Acc: 97.50% | Val Acc: 80.79% | Loss: 1.2280415934868179\n",
      "Epoch [95/125] | Train Acc: 97.60% | Val Acc: 80.49% | Loss: 1.2240403361724639\n",
      "Epoch [96/125] | Train Acc: 97.67% | Val Acc: 80.63% | Loss: 1.2208011063979372\n",
      "Epoch [97/125] | Train Acc: 97.79% | Val Acc: 80.78% | Loss: 1.2177384287885558\n",
      "Epoch [98/125] | Train Acc: 97.89% | Val Acc: 80.83% | Loss: 1.2133248781505845\n",
      "Epoch [99/125] | Train Acc: 98.02% | Val Acc: 81.09% | Loss: 1.2114566677758727\n",
      "Epoch [100/125] | Train Acc: 98.03% | Val Acc: 81.00% | Loss: 1.2096506229055113\n",
      "Epoch [101/125] | Train Acc: 98.10% | Val Acc: 81.09% | Loss: 1.2063747774537843\n",
      "Epoch [102/125] | Train Acc: 98.15% | Val Acc: 81.27% | Loss: 1.2041331861389062\n",
      "Epoch [103/125] | Train Acc: 98.20% | Val Acc: 81.41% | Loss: 1.2017716394347484\n",
      "Epoch [104/125] | Train Acc: 98.33% | Val Acc: 81.43% | Loss: 1.1987304577025624\n",
      "Epoch [105/125] | Train Acc: 98.29% | Val Acc: 81.40% | Loss: 1.1978250664977759\n",
      "Epoch [106/125] | Train Acc: 98.40% | Val Acc: 81.40% | Loss: 1.195085962232522\n",
      "Epoch [107/125] | Train Acc: 98.34% | Val Acc: 81.55% | Loss: 1.1947688085458932\n",
      "Epoch [108/125] | Train Acc: 98.53% | Val Acc: 81.59% | Loss: 1.1899880299915506\n",
      "Epoch [109/125] | Train Acc: 98.50% | Val Acc: 81.52% | Loss: 1.1899284112314659\n",
      "Epoch [110/125] | Train Acc: 98.57% | Val Acc: 81.68% | Loss: 1.187970963337865\n",
      "Epoch [111/125] | Train Acc: 98.61% | Val Acc: 81.82% | Loss: 1.1867801984900095\n",
      "Epoch [112/125] | Train Acc: 98.67% | Val Acc: 81.75% | Loss: 1.1851582439927721\n",
      "Epoch [113/125] | Train Acc: 98.68% | Val Acc: 81.79% | Loss: 1.1850632443532627\n",
      "Epoch [114/125] | Train Acc: 98.62% | Val Acc: 81.89% | Loss: 1.1837009781600658\n",
      "Epoch [115/125] | Train Acc: 98.67% | Val Acc: 81.79% | Loss: 1.1828875905622354\n",
      "Epoch [116/125] | Train Acc: 98.69% | Val Acc: 81.85% | Loss: 1.1822379255475566\n",
      "Epoch [117/125] | Train Acc: 98.69% | Val Acc: 81.86% | Loss: 1.1820368541006858\n",
      "Epoch [118/125] | Train Acc: 98.67% | Val Acc: 81.85% | Loss: 1.1819215072721703\n",
      "Epoch [119/125] | Train Acc: 98.75% | Val Acc: 81.91% | Loss: 1.1802082189053273\n",
      "Epoch [120/125] | Train Acc: 98.73% | Val Acc: 82.05% | Loss: 1.1786354203473381\n",
      "Epoch [121/125] | Train Acc: 98.81% | Val Acc: 81.97% | Loss: 1.178519507007098\n",
      "Epoch [122/125] | Train Acc: 98.79% | Val Acc: 81.89% | Loss: 1.1790256932238206\n",
      "Epoch [123/125] | Train Acc: 98.72% | Val Acc: 81.92% | Loss: 1.1797390724823076\n",
      "Epoch [124/125] | Train Acc: 98.76% | Val Acc: 81.96% | Loss: 1.1789309895616413\n",
      "Epoch [125/125] | Train Acc: 98.74% | Val Acc: 81.99% | Loss: 1.1798053879509314\n",
      "Fold 2 Best Validation Accuracy: 82.05%\n",
      "\n",
      "===== Fold 3/5 =====\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "234bab4b6a8b47459d396ec69b5b7f75"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/125] | Train Acc: 1.96% | Val Acc: 1.90% | Loss: 4.364548193190177\n",
      "Epoch [2/125] | Train Acc: 21.20% | Val Acc: 39.88% | Loss: 3.3912318716557923\n",
      "Epoch [3/125] | Train Acc: 34.26% | Val Acc: 48.06% | Loss: 2.916291303213715\n",
      "Epoch [4/125] | Train Acc: 41.62% | Val Acc: 52.58% | Loss: 2.6860221153670394\n",
      "Epoch [5/125] | Train Acc: 46.39% | Val Acc: 54.67% | Loss: 2.548887589002992\n",
      "Epoch [6/125] | Train Acc: 49.73% | Val Acc: 56.74% | Loss: 2.445974023058716\n",
      "Epoch [7/125] | Train Acc: 52.88% | Val Acc: 58.34% | Loss: 2.3589025006267135\n",
      "Epoch [8/125] | Train Acc: 55.84% | Val Acc: 61.25% | Loss: 2.2741844451753486\n",
      "Epoch [9/125] | Train Acc: 58.89% | Val Acc: 62.47% | Loss: 2.193562033180941\n",
      "Epoch [10/125] | Train Acc: 61.73% | Val Acc: 63.69% | Loss: 2.1125043389392424\n",
      "Epoch [11/125] | Train Acc: 64.36% | Val Acc: 65.53% | Loss: 2.040511078980438\n",
      "Epoch [12/125] | Train Acc: 66.59% | Val Acc: 66.29% | Loss: 1.9807314712685555\n",
      "Epoch [13/125] | Train Acc: 68.40% | Val Acc: 66.44% | Loss: 1.9384650002563997\n",
      "Epoch [14/125] | Train Acc: 70.50% | Val Acc: 68.00% | Loss: 1.8785293397805312\n",
      "Epoch [15/125] | Train Acc: 73.18% | Val Acc: 69.57% | Loss: 1.8208517908075137\n",
      "Epoch [16/125] | Train Acc: 74.71% | Val Acc: 69.50% | Loss: 1.7787740912619496\n",
      "Epoch [17/125] | Train Acc: 76.00% | Val Acc: 70.30% | Loss: 1.7471997387183897\n",
      "Epoch [18/125] | Train Acc: 77.31% | Val Acc: 70.43% | Loss: 1.719694650288749\n",
      "Epoch [19/125] | Train Acc: 78.06% | Val Acc: 71.18% | Loss: 1.6972323541437226\n",
      "Epoch [20/125] | Train Acc: 78.64% | Val Acc: 71.55% | Loss: 1.6807871765151696\n",
      "Epoch [21/125] | Train Acc: 79.64% | Val Acc: 71.41% | Loss: 1.6607501638920172\n",
      "Epoch [22/125] | Train Acc: 80.21% | Val Acc: 71.92% | Loss: 1.6485829887395302\n",
      "Epoch [23/125] | Train Acc: 80.55% | Val Acc: 71.51% | Loss: 1.6390436891085276\n",
      "Epoch [24/125] | Train Acc: 80.90% | Val Acc: 71.74% | Loss: 1.629039117704392\n",
      "Epoch [25/125] | Train Acc: 81.79% | Val Acc: 72.95% | Loss: 1.6125894062854138\n",
      "Epoch [26/125] | Train Acc: 81.95% | Val Acc: 73.14% | Loss: 1.607084767297455\n",
      "Epoch [27/125] | Train Acc: 82.33% | Val Acc: 72.93% | Loss: 1.5968784129764857\n",
      "Epoch [28/125] | Train Acc: 82.90% | Val Acc: 73.02% | Loss: 1.5884496467119305\n",
      "Epoch [29/125] | Train Acc: 83.19% | Val Acc: 72.71% | Loss: 1.5824758352731063\n",
      "Epoch [30/125] | Train Acc: 83.17% | Val Acc: 73.08% | Loss: 1.5814005998864007\n",
      "Epoch [31/125] | Train Acc: 83.75% | Val Acc: 73.65% | Loss: 1.5678992169419181\n",
      "Epoch [32/125] | Train Acc: 84.06% | Val Acc: 73.93% | Loss: 1.5630728721696123\n",
      "Epoch [33/125] | Train Acc: 84.40% | Val Acc: 74.02% | Loss: 1.5544552300843384\n",
      "Epoch [34/125] | Train Acc: 84.77% | Val Acc: 73.77% | Loss: 1.5470529341006904\n",
      "Epoch [35/125] | Train Acc: 84.84% | Val Acc: 74.04% | Loss: 1.5433521980391005\n",
      "Epoch [36/125] | Train Acc: 85.11% | Val Acc: 74.17% | Loss: 1.534364120745252\n",
      "Epoch [37/125] | Train Acc: 85.42% | Val Acc: 74.67% | Loss: 1.5290416044069441\n",
      "Epoch [38/125] | Train Acc: 85.67% | Val Acc: 74.76% | Loss: 1.5236861832071507\n",
      "Epoch [39/125] | Train Acc: 86.20% | Val Acc: 74.69% | Loss: 1.511024453394012\n",
      "Epoch [40/125] | Train Acc: 86.45% | Val Acc: 75.01% | Loss: 1.5074132938937639\n",
      "Epoch [41/125] | Train Acc: 86.42% | Val Acc: 74.79% | Loss: 1.50517008930824\n",
      "Epoch [42/125] | Train Acc: 86.99% | Val Acc: 75.11% | Loss: 1.4940709020398737\n",
      "Epoch [43/125] | Train Acc: 87.32% | Val Acc: 75.07% | Loss: 1.4868730943376638\n",
      "Epoch [44/125] | Train Acc: 87.61% | Val Acc: 75.07% | Loss: 1.4818752762724512\n",
      "Epoch [45/125] | Train Acc: 87.73% | Val Acc: 75.05% | Loss: 1.4777336289648308\n",
      "Epoch [46/125] | Train Acc: 88.04% | Val Acc: 74.97% | Loss: 1.4692311389240313\n",
      "Epoch [47/125] | Train Acc: 88.36% | Val Acc: 75.69% | Loss: 1.464065569795763\n",
      "Epoch [48/125] | Train Acc: 88.58% | Val Acc: 75.80% | Loss: 1.4592530176860996\n",
      "Epoch [49/125] | Train Acc: 88.91% | Val Acc: 75.97% | Loss: 1.4510185166422733\n",
      "Epoch [50/125] | Train Acc: 88.92% | Val Acc: 76.24% | Loss: 1.4480335705137628\n",
      "Epoch [51/125] | Train Acc: 89.23% | Val Acc: 75.86% | Loss: 1.4409993344427543\n",
      "Epoch [52/125] | Train Acc: 89.67% | Val Acc: 75.82% | Loss: 1.4324964333742638\n",
      "Epoch [53/125] | Train Acc: 90.03% | Val Acc: 76.15% | Loss: 1.4240360688117342\n",
      "Epoch [54/125] | Train Acc: 90.11% | Val Acc: 76.39% | Loss: 1.4213089843432882\n",
      "Epoch [55/125] | Train Acc: 90.38% | Val Acc: 76.37% | Loss: 1.4170164664792977\n",
      "Epoch [56/125] | Train Acc: 90.61% | Val Acc: 76.33% | Loss: 1.410215934763231\n",
      "Epoch [57/125] | Train Acc: 90.97% | Val Acc: 76.97% | Loss: 1.4045215781930014\n",
      "Epoch [58/125] | Train Acc: 91.05% | Val Acc: 76.50% | Loss: 1.3984981875648157\n",
      "Epoch [59/125] | Train Acc: 91.18% | Val Acc: 76.82% | Loss: 1.3960948128949457\n",
      "Epoch [60/125] | Train Acc: 91.44% | Val Acc: 76.99% | Loss: 1.3878644595751748\n",
      "Epoch [61/125] | Train Acc: 91.87% | Val Acc: 77.25% | Loss: 1.383957411200252\n",
      "Epoch [62/125] | Train Acc: 92.05% | Val Acc: 76.96% | Loss: 1.3768242222140095\n",
      "Epoch [63/125] | Train Acc: 92.22% | Val Acc: 77.32% | Loss: 1.3705860258614246\n",
      "Epoch [64/125] | Train Acc: 92.33% | Val Acc: 77.53% | Loss: 1.3677666403638487\n",
      "Epoch [65/125] | Train Acc: 92.64% | Val Acc: 77.46% | Loss: 1.3606286962575265\n",
      "Epoch [66/125] | Train Acc: 92.97% | Val Acc: 77.70% | Loss: 1.354764194324544\n",
      "Epoch [67/125] | Train Acc: 93.07% | Val Acc: 77.71% | Loss: 1.351343965230839\n",
      "Epoch [68/125] | Train Acc: 93.31% | Val Acc: 77.65% | Loss: 1.3434221879389432\n",
      "Epoch [69/125] | Train Acc: 93.53% | Val Acc: 78.13% | Loss: 1.3383180608214835\n",
      "Epoch [70/125] | Train Acc: 93.63% | Val Acc: 78.06% | Loss: 1.3338916464236703\n",
      "Epoch [71/125] | Train Acc: 93.81% | Val Acc: 78.38% | Loss: 1.330246519204436\n",
      "Epoch [72/125] | Train Acc: 94.04% | Val Acc: 78.60% | Loss: 1.3232394550345694\n",
      "Epoch [73/125] | Train Acc: 94.29% | Val Acc: 78.68% | Loss: 1.3189946991119674\n",
      "Epoch [74/125] | Train Acc: 94.36% | Val Acc: 78.78% | Loss: 1.3175516622808288\n",
      "Epoch [75/125] | Train Acc: 94.74% | Val Acc: 78.51% | Loss: 1.3104970074577962\n",
      "Epoch [76/125] | Train Acc: 94.98% | Val Acc: 78.90% | Loss: 1.301993909890562\n",
      "Epoch [77/125] | Train Acc: 95.03% | Val Acc: 79.21% | Loss: 1.3006937995035732\n",
      "Epoch [78/125] | Train Acc: 95.21% | Val Acc: 79.46% | Loss: 1.2945846791038853\n",
      "Epoch [79/125] | Train Acc: 95.40% | Val Acc: 79.52% | Loss: 1.289828557446498\n",
      "Epoch [80/125] | Train Acc: 95.51% | Val Acc: 79.28% | Loss: 1.2879640750384027\n",
      "Epoch [81/125] | Train Acc: 95.78% | Val Acc: 79.45% | Loss: 1.280667629098621\n",
      "Epoch [82/125] | Train Acc: 95.74% | Val Acc: 79.51% | Loss: 1.2780244902779407\n",
      "Epoch [83/125] | Train Acc: 96.04% | Val Acc: 79.85% | Loss: 1.2732099707508164\n",
      "Epoch [84/125] | Train Acc: 96.18% | Val Acc: 79.88% | Loss: 1.2685867290822206\n",
      "Epoch [85/125] | Train Acc: 96.33% | Val Acc: 79.82% | Loss: 1.263241985765473\n",
      "Epoch [86/125] | Train Acc: 96.44% | Val Acc: 80.29% | Loss: 1.2600779585498734\n",
      "Epoch [87/125] | Train Acc: 96.71% | Val Acc: 80.11% | Loss: 1.2540809816052654\n",
      "Epoch [88/125] | Train Acc: 96.77% | Val Acc: 80.09% | Loss: 1.2510766493799625\n",
      "Epoch [89/125] | Train Acc: 96.90% | Val Acc: 80.23% | Loss: 1.246547478615931\n",
      "Epoch [90/125] | Train Acc: 97.08% | Val Acc: 80.20% | Loss: 1.2416515953302578\n",
      "Epoch [91/125] | Train Acc: 97.09% | Val Acc: 80.34% | Loss: 1.23830917154261\n",
      "Epoch [92/125] | Train Acc: 97.30% | Val Acc: 80.58% | Loss: 1.2341667351230774\n",
      "Epoch [93/125] | Train Acc: 97.34% | Val Acc: 80.69% | Loss: 1.2327677902102374\n",
      "Epoch [94/125] | Train Acc: 97.46% | Val Acc: 80.73% | Loss: 1.2274849614082168\n",
      "Epoch [95/125] | Train Acc: 97.57% | Val Acc: 81.00% | Loss: 1.2236361275963947\n",
      "Epoch [96/125] | Train Acc: 97.68% | Val Acc: 80.75% | Loss: 1.2224698681512436\n",
      "Epoch [97/125] | Train Acc: 97.77% | Val Acc: 81.14% | Loss: 1.216794175572296\n",
      "Epoch [98/125] | Train Acc: 97.87% | Val Acc: 81.11% | Loss: 1.2148907086521896\n",
      "Epoch [99/125] | Train Acc: 98.00% | Val Acc: 81.38% | Loss: 1.2115397432184207\n",
      "Epoch [100/125] | Train Acc: 97.99% | Val Acc: 81.46% | Loss: 1.209772527073897\n",
      "Epoch [101/125] | Train Acc: 98.15% | Val Acc: 81.24% | Loss: 1.2056648634211533\n",
      "Epoch [102/125] | Train Acc: 98.05% | Val Acc: 81.17% | Loss: 1.2062750645882063\n",
      "Epoch [103/125] | Train Acc: 98.21% | Val Acc: 81.36% | Loss: 1.2021915058323984\n",
      "Epoch [104/125] | Train Acc: 98.27% | Val Acc: 81.55% | Loss: 1.199426185169938\n",
      "Epoch [105/125] | Train Acc: 98.39% | Val Acc: 81.73% | Loss: 1.1971676633456778\n",
      "Epoch [106/125] | Train Acc: 98.41% | Val Acc: 81.68% | Loss: 1.1968036675123932\n",
      "Epoch [107/125] | Train Acc: 98.47% | Val Acc: 81.75% | Loss: 1.1929474286854898\n",
      "Epoch [108/125] | Train Acc: 98.51% | Val Acc: 81.84% | Loss: 1.1918727916848715\n",
      "Epoch [109/125] | Train Acc: 98.47% | Val Acc: 81.99% | Loss: 1.1902843723637995\n",
      "Epoch [110/125] | Train Acc: 98.53% | Val Acc: 81.99% | Loss: 1.1893827073993057\n",
      "Epoch [111/125] | Train Acc: 98.57% | Val Acc: 81.98% | Loss: 1.1873492558422756\n",
      "Epoch [112/125] | Train Acc: 98.60% | Val Acc: 81.92% | Loss: 1.1871797355106457\n",
      "Epoch [113/125] | Train Acc: 98.69% | Val Acc: 82.06% | Loss: 1.1847365442976343\n",
      "Epoch [114/125] | Train Acc: 98.63% | Val Acc: 81.90% | Loss: 1.1840857477043496\n",
      "Epoch [115/125] | Train Acc: 98.65% | Val Acc: 81.99% | Loss: 1.182482264296911\n",
      "Epoch [116/125] | Train Acc: 98.70% | Val Acc: 81.94% | Loss: 1.1819195660232948\n",
      "Epoch [117/125] | Train Acc: 98.65% | Val Acc: 82.03% | Loss: 1.1816025078887382\n",
      "Epoch [118/125] | Train Acc: 98.82% | Val Acc: 82.13% | Loss: 1.1802422800071835\n",
      "Epoch [119/125] | Train Acc: 98.68% | Val Acc: 82.14% | Loss: 1.181371321452203\n",
      "Epoch [120/125] | Train Acc: 98.76% | Val Acc: 82.14% | Loss: 1.1793007625966676\n",
      "Epoch [121/125] | Train Acc: 98.77% | Val Acc: 82.20% | Loss: 1.1795035953661177\n",
      "Epoch [122/125] | Train Acc: 98.71% | Val Acc: 82.10% | Loss: 1.1812985615661853\n",
      "Epoch [123/125] | Train Acc: 98.80% | Val Acc: 82.08% | Loss: 1.1796365933181985\n",
      "Epoch [124/125] | Train Acc: 98.77% | Val Acc: 82.05% | Loss: 1.1799743059118557\n",
      "Epoch [125/125] | Train Acc: 98.70% | Val Acc: 82.14% | Loss: 1.1798038839280298\n",
      "Fold 3 Best Validation Accuracy: 82.20%\n",
      "\n",
      "===== Fold 4/5 =====\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a47e78d576d4f12b8fccc7375782586"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/125] | Train Acc: 1.97% | Val Acc: 1.93% | Loss: 4.365409058246256\n",
      "Epoch [2/125] | Train Acc: 21.23% | Val Acc: 39.53% | Loss: 3.3889893794918398\n",
      "Epoch [3/125] | Train Acc: 34.30% | Val Acc: 47.91% | Loss: 2.9141537012254646\n",
      "Epoch [4/125] | Train Acc: 41.71% | Val Acc: 52.26% | Loss: 2.688505468337542\n",
      "Epoch [5/125] | Train Acc: 46.19% | Val Acc: 55.18% | Loss: 2.5493432024891964\n",
      "Epoch [6/125] | Train Acc: 49.81% | Val Acc: 57.74% | Loss: 2.4459380714970167\n",
      "Epoch [7/125] | Train Acc: 52.60% | Val Acc: 59.50% | Loss: 2.3616297011061667\n",
      "Epoch [8/125] | Train Acc: 55.65% | Val Acc: 61.59% | Loss: 2.275896930963148\n",
      "Epoch [9/125] | Train Acc: 58.64% | Val Acc: 63.43% | Loss: 2.195141626813048\n",
      "Epoch [10/125] | Train Acc: 61.74% | Val Acc: 64.98% | Loss: 2.1144052772589115\n",
      "Epoch [11/125] | Train Acc: 64.30% | Val Acc: 65.75% | Loss: 2.0429595942075034\n",
      "Epoch [12/125] | Train Acc: 66.52% | Val Acc: 66.67% | Loss: 1.9829411277647042\n",
      "Epoch [13/125] | Train Acc: 68.43% | Val Acc: 67.03% | Loss: 1.9366120055643097\n",
      "Epoch [14/125] | Train Acc: 70.88% | Val Acc: 68.63% | Loss: 1.8751479596081964\n",
      "Epoch [15/125] | Train Acc: 73.11% | Val Acc: 69.41% | Loss: 1.819813484816315\n",
      "Epoch [16/125] | Train Acc: 74.69% | Val Acc: 70.23% | Loss: 1.7816358245190584\n",
      "Epoch [17/125] | Train Acc: 75.98% | Val Acc: 71.21% | Loss: 1.7489674189301838\n",
      "Epoch [18/125] | Train Acc: 77.09% | Val Acc: 71.99% | Loss: 1.7215552928407052\n",
      "Epoch [19/125] | Train Acc: 78.33% | Val Acc: 71.84% | Loss: 1.6959216112874886\n",
      "Epoch [20/125] | Train Acc: 78.77% | Val Acc: 72.28% | Loss: 1.6801453945212592\n",
      "Epoch [21/125] | Train Acc: 79.55% | Val Acc: 72.41% | Loss: 1.6634833678468404\n",
      "Epoch [22/125] | Train Acc: 80.19% | Val Acc: 72.39% | Loss: 1.6509388674703227\n",
      "Epoch [23/125] | Train Acc: 80.63% | Val Acc: 72.25% | Loss: 1.6408438558395142\n",
      "Epoch [24/125] | Train Acc: 81.21% | Val Acc: 72.90% | Loss: 1.6274919437141946\n",
      "Epoch [25/125] | Train Acc: 81.71% | Val Acc: 73.35% | Loss: 1.614665457115824\n",
      "Epoch [26/125] | Train Acc: 82.02% | Val Acc: 73.31% | Loss: 1.6095908220357247\n",
      "Epoch [27/125] | Train Acc: 82.28% | Val Acc: 74.17% | Loss: 1.5992593826254693\n",
      "Epoch [28/125] | Train Acc: 82.61% | Val Acc: 73.58% | Loss: 1.5905664797704653\n",
      "Epoch [29/125] | Train Acc: 82.88% | Val Acc: 73.56% | Loss: 1.586556093921966\n",
      "Epoch [30/125] | Train Acc: 83.23% | Val Acc: 73.90% | Loss: 1.5795391763660793\n",
      "Epoch [31/125] | Train Acc: 83.45% | Val Acc: 74.23% | Loss: 1.572466633802115\n",
      "Epoch [32/125] | Train Acc: 83.94% | Val Acc: 74.38% | Loss: 1.5624166038021499\n",
      "Epoch [33/125] | Train Acc: 84.28% | Val Acc: 74.34% | Loss: 1.5554558690311526\n",
      "Epoch [34/125] | Train Acc: 84.81% | Val Acc: 74.61% | Loss: 1.54518298213152\n",
      "Epoch [35/125] | Train Acc: 84.83% | Val Acc: 74.47% | Loss: 1.5421046483403713\n",
      "Epoch [36/125] | Train Acc: 85.07% | Val Acc: 74.98% | Loss: 1.536645098520171\n",
      "Epoch [37/125] | Train Acc: 85.49% | Val Acc: 74.87% | Loss: 1.53055298756688\n",
      "Epoch [38/125] | Train Acc: 85.85% | Val Acc: 75.12% | Loss: 1.519812862288813\n",
      "Epoch [39/125] | Train Acc: 85.99% | Val Acc: 75.40% | Loss: 1.516400545325797\n",
      "Epoch [40/125] | Train Acc: 86.38% | Val Acc: 75.44% | Loss: 1.5084725745919272\n",
      "Epoch [41/125] | Train Acc: 86.81% | Val Acc: 75.49% | Loss: 1.4988234391872262\n",
      "Epoch [42/125] | Train Acc: 86.92% | Val Acc: 75.24% | Loss: 1.495351287527779\n",
      "Epoch [43/125] | Train Acc: 87.17% | Val Acc: 75.71% | Loss: 1.4901598522897468\n",
      "Epoch [44/125] | Train Acc: 87.31% | Val Acc: 75.53% | Loss: 1.487098441646895\n",
      "Epoch [45/125] | Train Acc: 87.67% | Val Acc: 75.83% | Loss: 1.477969498901615\n",
      "Epoch [46/125] | Train Acc: 87.99% | Val Acc: 76.26% | Loss: 1.4698570552906498\n",
      "Epoch [47/125] | Train Acc: 88.32% | Val Acc: 76.75% | Loss: 1.4667595357195071\n",
      "Epoch [48/125] | Train Acc: 88.83% | Val Acc: 76.63% | Loss: 1.455699841522312\n",
      "Epoch [49/125] | Train Acc: 88.99% | Val Acc: 76.44% | Loss: 1.451443648555238\n",
      "Epoch [50/125] | Train Acc: 89.03% | Val Acc: 76.81% | Loss: 1.4468908863496304\n",
      "Epoch [51/125] | Train Acc: 89.33% | Val Acc: 76.82% | Loss: 1.4405526618663127\n",
      "Epoch [52/125] | Train Acc: 89.69% | Val Acc: 76.88% | Loss: 1.4326365172181903\n",
      "Epoch [53/125] | Train Acc: 89.86% | Val Acc: 77.31% | Loss: 1.4293307831911675\n",
      "Epoch [54/125] | Train Acc: 90.03% | Val Acc: 77.55% | Loss: 1.4238628563234033\n",
      "Epoch [55/125] | Train Acc: 90.26% | Val Acc: 77.27% | Loss: 1.4201207048667661\n",
      "Epoch [56/125] | Train Acc: 90.58% | Val Acc: 77.57% | Loss: 1.4117175430015856\n",
      "Epoch [57/125] | Train Acc: 90.72% | Val Acc: 77.42% | Loss: 1.4059716494684453\n",
      "Epoch [58/125] | Train Acc: 91.06% | Val Acc: 77.53% | Loss: 1.3986180783105742\n",
      "Epoch [59/125] | Train Acc: 91.22% | Val Acc: 77.89% | Loss: 1.3957785321263287\n",
      "Epoch [60/125] | Train Acc: 91.72% | Val Acc: 78.03% | Loss: 1.3866201832737988\n",
      "Epoch [61/125] | Train Acc: 91.64% | Val Acc: 78.07% | Loss: 1.384211720289204\n",
      "Epoch [62/125] | Train Acc: 91.95% | Val Acc: 78.04% | Loss: 1.3787432630191867\n",
      "Epoch [63/125] | Train Acc: 92.13% | Val Acc: 78.53% | Loss: 1.3724846476602128\n",
      "Epoch [64/125] | Train Acc: 92.52% | Val Acc: 78.71% | Loss: 1.3654165267686098\n",
      "Epoch [65/125] | Train Acc: 92.60% | Val Acc: 78.49% | Loss: 1.3636551761097808\n",
      "Epoch [66/125] | Train Acc: 92.82% | Val Acc: 78.57% | Loss: 1.3563405848285417\n",
      "Epoch [67/125] | Train Acc: 93.14% | Val Acc: 78.48% | Loss: 1.3505145902798938\n",
      "Epoch [68/125] | Train Acc: 93.37% | Val Acc: 78.30% | Loss: 1.3443438270567816\n",
      "Epoch [69/125] | Train Acc: 93.42% | Val Acc: 78.66% | Loss: 1.3415597195319293\n",
      "Epoch [70/125] | Train Acc: 93.74% | Val Acc: 79.03% | Loss: 1.334311499541412\n",
      "Epoch [71/125] | Train Acc: 93.85% | Val Acc: 79.43% | Loss: 1.330650789778374\n",
      "Epoch [72/125] | Train Acc: 94.13% | Val Acc: 79.13% | Loss: 1.3248316768498527\n",
      "Epoch [73/125] | Train Acc: 94.25% | Val Acc: 79.52% | Loss: 1.3220680180184732\n",
      "Epoch [74/125] | Train Acc: 94.62% | Val Acc: 79.57% | Loss: 1.3137561013502963\n",
      "Epoch [75/125] | Train Acc: 94.86% | Val Acc: 79.66% | Loss: 1.3076409880797972\n",
      "Epoch [76/125] | Train Acc: 94.75% | Val Acc: 79.49% | Loss: 1.3090336153767408\n",
      "Epoch [77/125] | Train Acc: 95.10% | Val Acc: 79.81% | Loss: 1.299490112438455\n",
      "Epoch [78/125] | Train Acc: 95.34% | Val Acc: 79.82% | Loss: 1.2953840747939647\n",
      "Epoch [79/125] | Train Acc: 95.44% | Val Acc: 79.90% | Loss: 1.2935922881629194\n",
      "Epoch [80/125] | Train Acc: 95.64% | Val Acc: 80.04% | Loss: 1.2856980909179163\n",
      "Epoch [81/125] | Train Acc: 95.86% | Val Acc: 80.24% | Loss: 1.2813563910080688\n",
      "Epoch [82/125] | Train Acc: 95.89% | Val Acc: 80.27% | Loss: 1.2790011913770587\n",
      "Epoch [83/125] | Train Acc: 96.08% | Val Acc: 80.41% | Loss: 1.2728397817105548\n",
      "Epoch [84/125] | Train Acc: 96.24% | Val Acc: 80.76% | Loss: 1.2704003218974391\n",
      "Epoch [85/125] | Train Acc: 96.33% | Val Acc: 80.69% | Loss: 1.265464649130715\n",
      "Epoch [86/125] | Train Acc: 96.39% | Val Acc: 80.76% | Loss: 1.2606439086295322\n",
      "Epoch [87/125] | Train Acc: 96.62% | Val Acc: 81.01% | Loss: 1.2558779770308577\n",
      "Epoch [88/125] | Train Acc: 96.72% | Val Acc: 80.87% | Loss: 1.252724962704491\n",
      "Epoch [89/125] | Train Acc: 96.90% | Val Acc: 81.11% | Loss: 1.2496233096717981\n",
      "Epoch [90/125] | Train Acc: 97.01% | Val Acc: 81.17% | Loss: 1.2432294530194528\n",
      "Epoch [91/125] | Train Acc: 97.25% | Val Acc: 81.28% | Loss: 1.2386904201241582\n",
      "Epoch [92/125] | Train Acc: 97.19% | Val Acc: 81.25% | Loss: 1.237474983751435\n",
      "Epoch [93/125] | Train Acc: 97.36% | Val Acc: 81.62% | Loss: 1.233247538157798\n",
      "Epoch [94/125] | Train Acc: 97.56% | Val Acc: 81.59% | Loss: 1.2290108829689903\n",
      "Epoch [95/125] | Train Acc: 97.61% | Val Acc: 81.45% | Loss: 1.2257329367894065\n",
      "Epoch [96/125] | Train Acc: 97.67% | Val Acc: 81.57% | Loss: 1.2231681980971816\n",
      "Epoch [97/125] | Train Acc: 97.75% | Val Acc: 81.72% | Loss: 1.2204965009335544\n",
      "Epoch [98/125] | Train Acc: 97.87% | Val Acc: 81.81% | Loss: 1.2169596141701045\n",
      "Epoch [99/125] | Train Acc: 97.89% | Val Acc: 81.94% | Loss: 1.2143899350627276\n",
      "Epoch [100/125] | Train Acc: 97.99% | Val Acc: 82.12% | Loss: 1.2115521898174104\n",
      "Epoch [101/125] | Train Acc: 98.11% | Val Acc: 82.08% | Loss: 1.2091714589382103\n",
      "Epoch [102/125] | Train Acc: 98.08% | Val Acc: 82.30% | Loss: 1.207020553408875\n",
      "Epoch [103/125] | Train Acc: 98.25% | Val Acc: 82.17% | Loss: 1.203024159291234\n",
      "Epoch [104/125] | Train Acc: 98.28% | Val Acc: 82.41% | Loss: 1.201304128297196\n",
      "Epoch [105/125] | Train Acc: 98.39% | Val Acc: 82.43% | Loss: 1.19869798958931\n",
      "Epoch [106/125] | Train Acc: 98.41% | Val Acc: 82.48% | Loss: 1.1969508915686071\n",
      "Epoch [107/125] | Train Acc: 98.39% | Val Acc: 82.48% | Loss: 1.1954620015818092\n",
      "Epoch [108/125] | Train Acc: 98.47% | Val Acc: 82.46% | Loss: 1.1925735655742513\n",
      "Epoch [109/125] | Train Acc: 98.50% | Val Acc: 82.46% | Loss: 1.1918011622024751\n",
      "Epoch [110/125] | Train Acc: 98.50% | Val Acc: 82.59% | Loss: 1.190385391743823\n",
      "Epoch [111/125] | Train Acc: 98.56% | Val Acc: 82.56% | Loss: 1.1888741926936448\n",
      "Epoch [112/125] | Train Acc: 98.64% | Val Acc: 82.64% | Loss: 1.1882264310765518\n",
      "Epoch [113/125] | Train Acc: 98.63% | Val Acc: 82.59% | Loss: 1.186542334733473\n",
      "Epoch [114/125] | Train Acc: 98.66% | Val Acc: 82.71% | Loss: 1.185501743384309\n",
      "Epoch [115/125] | Train Acc: 98.72% | Val Acc: 82.76% | Loss: 1.18384374402643\n",
      "Epoch [116/125] | Train Acc: 98.67% | Val Acc: 82.76% | Loss: 1.1841060439906985\n",
      "Epoch [117/125] | Train Acc: 98.70% | Val Acc: 82.70% | Loss: 1.1833480416627682\n",
      "Epoch [118/125] | Train Acc: 98.72% | Val Acc: 82.71% | Loss: 1.1818774054220238\n",
      "Epoch [119/125] | Train Acc: 98.73% | Val Acc: 82.77% | Loss: 1.181505588680756\n",
      "Epoch [120/125] | Train Acc: 98.75% | Val Acc: 82.81% | Loss: 1.1810870295709754\n",
      "Epoch [121/125] | Train Acc: 98.76% | Val Acc: 82.74% | Loss: 1.179762457074946\n",
      "Epoch [122/125] | Train Acc: 98.70% | Val Acc: 82.80% | Loss: 1.1822620556266077\n",
      "Epoch [123/125] | Train Acc: 98.75% | Val Acc: 82.90% | Loss: 1.1815804924139832\n",
      "Epoch [124/125] | Train Acc: 98.71% | Val Acc: 82.77% | Loss: 1.18162283495581\n",
      "Epoch [125/125] | Train Acc: 98.77% | Val Acc: 82.84% | Loss: 1.1806463349417493\n",
      "Fold 4 Best Validation Accuracy: 82.90%\n",
      "\n",
      "===== Fold 5/5 =====\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15812090a26d4f1f95fa1ff8816a3c01"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/125] | Train Acc: 1.97% | Val Acc: 2.12% | Loss: 4.364784382176841\n",
      "Epoch [2/125] | Train Acc: 21.29% | Val Acc: 39.32% | Loss: 3.388981940960028\n",
      "Epoch [3/125] | Train Acc: 34.07% | Val Acc: 47.41% | Loss: 2.9163020414298124\n",
      "Epoch [4/125] | Train Acc: 41.76% | Val Acc: 51.95% | Loss: 2.685524843577976\n",
      "Epoch [5/125] | Train Acc: 46.34% | Val Acc: 54.93% | Loss: 2.5500038021240194\n",
      "Epoch [6/125] | Train Acc: 49.74% | Val Acc: 57.60% | Loss: 2.4480302714169384\n",
      "Epoch [7/125] | Train Acc: 52.87% | Val Acc: 59.51% | Loss: 2.3593630921070594\n",
      "Epoch [8/125] | Train Acc: 55.84% | Val Acc: 60.96% | Loss: 2.2769150534378975\n",
      "Epoch [9/125] | Train Acc: 58.53% | Val Acc: 62.79% | Loss: 2.1966774172667414\n",
      "Epoch [10/125] | Train Acc: 61.67% | Val Acc: 64.30% | Loss: 2.1155038865185567\n",
      "Epoch [11/125] | Train Acc: 64.37% | Val Acc: 65.71% | Loss: 2.0449028108620024\n",
      "Epoch [12/125] | Train Acc: 66.83% | Val Acc: 66.92% | Loss: 1.9789001054570836\n",
      "Epoch [13/125] | Train Acc: 68.32% | Val Acc: 67.41% | Loss: 1.937910286280421\n",
      "Epoch [14/125] | Train Acc: 70.64% | Val Acc: 68.89% | Loss: 1.8788119868644215\n",
      "Epoch [15/125] | Train Acc: 72.88% | Val Acc: 69.38% | Loss: 1.8234654190862496\n",
      "Epoch [16/125] | Train Acc: 74.73% | Val Acc: 69.57% | Loss: 1.7805692712829182\n",
      "Epoch [17/125] | Train Acc: 76.02% | Val Acc: 70.84% | Loss: 1.7478653405930646\n",
      "Epoch [18/125] | Train Acc: 77.09% | Val Acc: 70.83% | Loss: 1.722763283228648\n",
      "Epoch [19/125] | Train Acc: 77.83% | Val Acc: 71.28% | Loss: 1.700363651599118\n",
      "Epoch [20/125] | Train Acc: 78.82% | Val Acc: 71.56% | Loss: 1.6830967273793143\n",
      "Epoch [21/125] | Train Acc: 79.52% | Val Acc: 71.95% | Loss: 1.6613315967147704\n",
      "Epoch [22/125] | Train Acc: 80.18% | Val Acc: 71.98% | Loss: 1.6500745049579053\n",
      "Epoch [23/125] | Train Acc: 80.66% | Val Acc: 72.91% | Loss: 1.638155877912981\n",
      "Epoch [24/125] | Train Acc: 81.12% | Val Acc: 72.89% | Loss: 1.625184770793757\n",
      "Epoch [25/125] | Train Acc: 81.67% | Val Acc: 73.13% | Loss: 1.6140493144970833\n",
      "Epoch [26/125] | Train Acc: 81.80% | Val Acc: 72.76% | Loss: 1.609487924148778\n",
      "Epoch [27/125] | Train Acc: 82.17% | Val Acc: 73.58% | Loss: 1.6012258297659216\n",
      "Epoch [28/125] | Train Acc: 82.61% | Val Acc: 73.20% | Loss: 1.5914011835809194\n",
      "Epoch [29/125] | Train Acc: 83.13% | Val Acc: 73.68% | Loss: 1.5819094831518654\n",
      "Epoch [30/125] | Train Acc: 83.49% | Val Acc: 73.50% | Loss: 1.5741861253650147\n",
      "Epoch [31/125] | Train Acc: 83.69% | Val Acc: 73.83% | Loss: 1.5672751000609175\n",
      "Epoch [32/125] | Train Acc: 84.05% | Val Acc: 74.27% | Loss: 1.562142931595351\n",
      "Epoch [33/125] | Train Acc: 84.36% | Val Acc: 74.21% | Loss: 1.5570476796504018\n",
      "Epoch [34/125] | Train Acc: 84.69% | Val Acc: 74.47% | Loss: 1.547241561742637\n",
      "Epoch [35/125] | Train Acc: 84.96% | Val Acc: 74.35% | Loss: 1.5396401189981994\n",
      "Epoch [36/125] | Train Acc: 85.12% | Val Acc: 74.88% | Loss: 1.5395206974034168\n",
      "Epoch [37/125] | Train Acc: 85.31% | Val Acc: 74.64% | Loss: 1.5321304807009717\n",
      "Epoch [38/125] | Train Acc: 85.67% | Val Acc: 74.92% | Loss: 1.522897114616752\n",
      "Epoch [39/125] | Train Acc: 86.01% | Val Acc: 75.07% | Loss: 1.5160088577756414\n",
      "Epoch [40/125] | Train Acc: 86.37% | Val Acc: 75.06% | Loss: 1.5095334824560875\n",
      "Epoch [41/125] | Train Acc: 86.55% | Val Acc: 75.51% | Loss: 1.5052224454317416\n",
      "Epoch [42/125] | Train Acc: 86.86% | Val Acc: 75.73% | Loss: 1.4968232599630455\n",
      "Epoch [43/125] | Train Acc: 87.35% | Val Acc: 75.42% | Loss: 1.4867383677926078\n",
      "Epoch [44/125] | Train Acc: 87.42% | Val Acc: 75.94% | Loss: 1.4841319420407815\n",
      "Epoch [45/125] | Train Acc: 87.67% | Val Acc: 75.85% | Loss: 1.4787373723347872\n",
      "Epoch [46/125] | Train Acc: 88.06% | Val Acc: 76.19% | Loss: 1.4681349444247827\n",
      "Epoch [47/125] | Train Acc: 88.29% | Val Acc: 76.05% | Loss: 1.4662456055777484\n",
      "Epoch [48/125] | Train Acc: 88.46% | Val Acc: 76.45% | Loss: 1.4600600772773542\n",
      "Epoch [49/125] | Train Acc: 88.79% | Val Acc: 76.35% | Loss: 1.4533877716822237\n",
      "Epoch [50/125] | Train Acc: 89.05% | Val Acc: 76.59% | Loss: 1.4476696132637434\n",
      "Epoch [51/125] | Train Acc: 89.26% | Val Acc: 76.36% | Loss: 1.4404980668234073\n",
      "Epoch [52/125] | Train Acc: 89.75% | Val Acc: 76.96% | Loss: 1.4336182360095233\n",
      "Epoch [53/125] | Train Acc: 89.78% | Val Acc: 76.55% | Loss: 1.4279713594522505\n",
      "Epoch [54/125] | Train Acc: 90.08% | Val Acc: 77.05% | Loss: 1.4219871978661427\n",
      "Epoch [55/125] | Train Acc: 90.43% | Val Acc: 76.90% | Loss: 1.41564345030803\n",
      "Epoch [56/125] | Train Acc: 90.47% | Val Acc: 77.42% | Loss: 1.4114292673426057\n",
      "Epoch [57/125] | Train Acc: 90.86% | Val Acc: 77.42% | Loss: 1.4041195638485227\n",
      "Epoch [58/125] | Train Acc: 91.07% | Val Acc: 77.40% | Loss: 1.3989643024037839\n",
      "Epoch [59/125] | Train Acc: 91.48% | Val Acc: 77.77% | Loss: 1.3896778489123969\n",
      "Epoch [60/125] | Train Acc: 91.43% | Val Acc: 77.75% | Loss: 1.3883321951716618\n",
      "Epoch [61/125] | Train Acc: 91.66% | Val Acc: 77.88% | Loss: 1.3845556640660122\n",
      "Epoch [62/125] | Train Acc: 92.02% | Val Acc: 78.07% | Loss: 1.3767797607807626\n",
      "Epoch [63/125] | Train Acc: 91.96% | Val Acc: 77.96% | Loss: 1.3740701772358774\n",
      "Epoch [64/125] | Train Acc: 92.35% | Val Acc: 78.51% | Loss: 1.3686065296227885\n",
      "Epoch [65/125] | Train Acc: 92.69% | Val Acc: 78.15% | Loss: 1.3605053151616047\n",
      "Epoch [66/125] | Train Acc: 92.78% | Val Acc: 78.41% | Loss: 1.3566377011294133\n",
      "Epoch [67/125] | Train Acc: 93.18% | Val Acc: 78.35% | Loss: 1.3492328893383756\n",
      "Epoch [68/125] | Train Acc: 93.54% | Val Acc: 78.78% | Loss: 1.3435541945609883\n",
      "Epoch [69/125] | Train Acc: 93.48% | Val Acc: 78.45% | Loss: 1.340999851177844\n",
      "Epoch [70/125] | Train Acc: 93.74% | Val Acc: 79.16% | Loss: 1.3344173341528556\n",
      "Epoch [71/125] | Train Acc: 93.90% | Val Acc: 78.68% | Loss: 1.330161793564269\n",
      "Epoch [72/125] | Train Acc: 94.11% | Val Acc: 79.21% | Loss: 1.324401219942392\n",
      "Epoch [73/125] | Train Acc: 94.14% | Val Acc: 79.23% | Loss: 1.3216727500327317\n",
      "Epoch [74/125] | Train Acc: 94.41% | Val Acc: 79.11% | Loss: 1.3144270245441454\n",
      "Epoch [75/125] | Train Acc: 94.70% | Val Acc: 79.32% | Loss: 1.3094128784658818\n",
      "Epoch [76/125] | Train Acc: 94.83% | Val Acc: 79.35% | Loss: 1.3049893321365404\n",
      "Epoch [77/125] | Train Acc: 95.04% | Val Acc: 79.85% | Loss: 1.2993241152654493\n",
      "Epoch [78/125] | Train Acc: 95.27% | Val Acc: 79.61% | Loss: 1.2945924923834276\n",
      "Epoch [79/125] | Train Acc: 95.35% | Val Acc: 79.66% | Loss: 1.2898176018852205\n",
      "Epoch [80/125] | Train Acc: 95.56% | Val Acc: 79.99% | Loss: 1.283958213978327\n",
      "Epoch [81/125] | Train Acc: 95.64% | Val Acc: 79.72% | Loss: 1.283854127420895\n",
      "Epoch [82/125] | Train Acc: 95.90% | Val Acc: 80.21% | Loss: 1.2756339670709904\n",
      "Epoch [83/125] | Train Acc: 96.04% | Val Acc: 80.26% | Loss: 1.2726233061820258\n",
      "Epoch [84/125] | Train Acc: 96.10% | Val Acc: 80.24% | Loss: 1.2685843488433675\n",
      "Epoch [85/125] | Train Acc: 96.28% | Val Acc: 80.34% | Loss: 1.2630215917187895\n",
      "Epoch [86/125] | Train Acc: 96.48% | Val Acc: 80.35% | Loss: 1.2593914588502475\n",
      "Epoch [87/125] | Train Acc: 96.66% | Val Acc: 80.57% | Loss: 1.2551890636697953\n",
      "Epoch [88/125] | Train Acc: 96.76% | Val Acc: 80.45% | Loss: 1.2511443546753058\n",
      "Epoch [89/125] | Train Acc: 96.85% | Val Acc: 80.95% | Loss: 1.2471635850379394\n",
      "Epoch [90/125] | Train Acc: 96.97% | Val Acc: 80.68% | Loss: 1.2431271398797263\n",
      "Epoch [91/125] | Train Acc: 97.17% | Val Acc: 80.90% | Loss: 1.2392021083505371\n",
      "Epoch [92/125] | Train Acc: 97.31% | Val Acc: 81.15% | Loss: 1.2346342222089706\n",
      "Epoch [93/125] | Train Acc: 97.28% | Val Acc: 81.14% | Loss: 1.2321690155849223\n",
      "Epoch [94/125] | Train Acc: 97.45% | Val Acc: 81.04% | Loss: 1.2285611500055609\n",
      "Epoch [95/125] | Train Acc: 97.55% | Val Acc: 81.27% | Loss: 1.224713765729378\n",
      "Epoch [96/125] | Train Acc: 97.69% | Val Acc: 81.55% | Loss: 1.2217981020161919\n",
      "Epoch [97/125] | Train Acc: 97.82% | Val Acc: 81.46% | Loss: 1.217486037212643\n",
      "Epoch [98/125] | Train Acc: 97.87% | Val Acc: 81.51% | Loss: 1.2146914983910284\n",
      "Epoch [99/125] | Train Acc: 97.91% | Val Acc: 81.64% | Loss: 1.2153330328332186\n",
      "Epoch [100/125] | Train Acc: 97.96% | Val Acc: 81.72% | Loss: 1.2103213046463837\n",
      "Epoch [101/125] | Train Acc: 98.11% | Val Acc: 81.98% | Loss: 1.2062663907045343\n",
      "Epoch [102/125] | Train Acc: 98.10% | Val Acc: 81.96% | Loss: 1.2043653952797024\n",
      "Epoch [103/125] | Train Acc: 98.22% | Val Acc: 81.91% | Loss: 1.201299710691267\n",
      "Epoch [104/125] | Train Acc: 98.24% | Val Acc: 82.12% | Loss: 1.1999097458540806\n",
      "Epoch [105/125] | Train Acc: 98.32% | Val Acc: 81.97% | Loss: 1.1983684265274226\n",
      "Epoch [106/125] | Train Acc: 98.34% | Val Acc: 82.09% | Loss: 1.195113292961795\n",
      "Epoch [107/125] | Train Acc: 98.43% | Val Acc: 82.19% | Loss: 1.194375028044578\n",
      "Epoch [108/125] | Train Acc: 98.45% | Val Acc: 82.26% | Loss: 1.1920204587662624\n",
      "Epoch [109/125] | Train Acc: 98.47% | Val Acc: 82.28% | Loss: 1.1906725451518714\n",
      "Epoch [110/125] | Train Acc: 98.58% | Val Acc: 82.34% | Loss: 1.1892234951375205\n",
      "Epoch [111/125] | Train Acc: 98.52% | Val Acc: 82.30% | Loss: 1.1878205024619164\n",
      "Epoch [112/125] | Train Acc: 98.59% | Val Acc: 82.49% | Loss: 1.1862809379046775\n",
      "Epoch [113/125] | Train Acc: 98.64% | Val Acc: 82.53% | Loss: 1.1847366522856393\n",
      "Epoch [114/125] | Train Acc: 98.65% | Val Acc: 82.63% | Loss: 1.1830060243565248\n",
      "Epoch [115/125] | Train Acc: 98.72% | Val Acc: 82.50% | Loss: 1.1831845911385555\n",
      "Epoch [116/125] | Train Acc: 98.69% | Val Acc: 82.56% | Loss: 1.1823232571456137\n",
      "Epoch [117/125] | Train Acc: 98.69% | Val Acc: 82.54% | Loss: 1.1818467473287546\n",
      "Epoch [118/125] | Train Acc: 98.69% | Val Acc: 82.48% | Loss: 1.1816654788379182\n",
      "Epoch [119/125] | Train Acc: 98.73% | Val Acc: 82.65% | Loss: 1.1795785451729714\n",
      "Epoch [120/125] | Train Acc: 98.70% | Val Acc: 82.58% | Loss: 1.1807415794083576\n",
      "Epoch [121/125] | Train Acc: 98.72% | Val Acc: 82.65% | Loss: 1.1812013050623678\n",
      "Epoch [122/125] | Train Acc: 98.72% | Val Acc: 82.64% | Loss: 1.1803258595167674\n",
      "Epoch [123/125] | Train Acc: 98.71% | Val Acc: 82.64% | Loss: 1.1804969124620261\n",
      "Epoch [124/125] | Train Acc: 98.75% | Val Acc: 82.65% | Loss: 1.1794821952989478\n",
      "Epoch [125/125] | Train Acc: 98.73% | Val Acc: 82.62% | Loss: 1.1816987052677999\n",
      "Fold 5 Best Validation Accuracy: 82.65%\n",
      "\n",
      "K-Fold Validation Results (5 folds): Mean = 82.38%, Std = 0.38%\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d42841c5f94cadb2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
